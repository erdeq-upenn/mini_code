大家好,今天我们就来过一下多模态的串讲,其实之前我们也讲了很多工作了,比如说CLIP,还有ViLT,以及CLIP的那么多后续工作,多模态学习在最近几年真的是异常的火爆,除了普通的这种多模态学习,比如说视觉问答、图文检索这些,其实之前讲的所有这种Language Guided Detection,或者Language Guided Segmentation,这些任务它都是多模态,而且包括最近大火的这种文本图像生成,或者文本视频生成,我们耳熟能详的DALL·E 2、Stable Diffusion,Phenaki Video、Imagen Video,所有的这些统统都属于多模态学习,但我们这期多模态串讲,可能更多的还是讲传统的这种多模态学习,也就是说当下游任务是这种图文检索,就是Image Text Retrieval,或者VQA,就是视觉问答,或者Visual Reasoning,就是视觉推理,还有就是Visual Entailment,视觉蕴含,就是这些传统的多模态任务,但即使如此,相关的工作也是多的数不胜数,我们这个串讲的第一部分,应该就是讲一下只用Transformer Encoder的一些方法,比如说之前的这个ViLT、CLIP,还有我们今天要讲的ALBEF和VLMo,然后在第二部分呢,我们会讲到用Transformer Encoder和Decoder一起的一些方法,比如说BLIP、CoCa、BEIT v3以及PaLI,在我们开始讲ALBEF之前,我们先简单回顾一下ViLT和CLIP,鉴于ViLT论文里这个图二真的是总结的比较好,我们就先从这张图开始,ViLT那篇论文的研究动机,其实就是为了把目标检测从视觉端给拿掉,原因很简单,因为你用一个预训练的目标检测器,去抽视觉特征的时候,它就会面临很多很多的局限性,我们之前也都讲过了,但是直到21年之前,也就是在Vision Transformer出现之前,很难有什么很好的办法,能把这个预训练的目标检测器给移除掉,所以说最早期的工作,比如说分类 (a) 就有一些VSE或者VSE++的工作,它们的文本端就是直接抽一个文本特征,但是它们的视觉端就非常的大,也就是说它需要的计算量非常的多,因为它是一个目标检测器,然后当得到了文本特征和视觉特征之后,它最后只能做一个很简单的模态之间的交互,从而去做这种多模态的任务,后续这些工作也就是分类 (c) 里的工作,就像我们耳熟能详的OSCAR或者ViLBERT、UNITER,他们发现对于这种多模态的任务来说,最后的这个模态之间的交互是非常重要的,只有有了这个模态之间更深层的交互,对于这种VQA、VR、VE这些任务来说,效果才会非常的好,所以他们就把最初的这种简单的点乘的,这种模态之间的交互,就变成了一个Transformer的Encoder,或者变成别的更复杂的模型结构,去做这种模态之间的交互,所以这些方法的性能都非常非常的好,但是随之而来的缺点也很明显,就是说所有的这一系列的工作,它通通都用了这个预训练的目标检测器,而且再加上这么一个更大的模态融合的部分,这个模型不论是训练还是部署都非常的困难,所以说当Vision Transformer出来之后,ViLT这篇论文也就应运而生了,因为他们发现在Vision Transformer里,这种基于Patch的视觉特征,其实跟之前这种基于Bounding Box的视觉特征,也没什么太大的区别,它也能很好的拿来做这种图片分类,或者目标检测的任务,这样我们就可以把这么大的,一个预训练好的目标检测器,直接就换成一层Patch Embedding,就能去抽取这个视觉的特征了,所以大大的降低了这个运算复杂度,尤其是在做推理的时候,但是如果你的文本特征,只是简单的Tokenize一下,视觉特征也只是简单的Patch Embedding一下,那肯定是远远不够的,所以对于多模态任务来说,这个后面的模态融合非常关键,所以ViLT就把之前 (c) 类里的这些方法,这个模态融合的方法直接借鉴了过来,用一个很大的Transformer Encoder,去做这种模态融合,从而达到了还不错的效果,因为移除掉了预训练的目标检测器,而换成了可以学习的Patch Embedding Layer,所以说ViLT模型极其的简单,它虽然作为一个多模态学习的框架,但其实就跟NLP那边的框架没什么区别,无非就是先都Tokenized了一下,然后直接扔给一个Transformer去学习了,所以非常的简单易学,但是ViLT也有它自己的缺点,首先第一个缺点就是它的性能不够高,ViLT在很多任务上是比不过 (c) 类里的这些方法的,原因之前其实我们也讲过很多遍了,对于现有的这些多模态任务而言,有可能是这个数据集的bias,也有可能是这个任务就需要更多的视觉能力,但总之我们是需要更强的这个视觉部分,简单来说就是我们视觉的那个模型,应该要比文本的那个模型要大,最后的这个效果才能好,但是在ViLT里,文本端用的Tokenizer其实是很好的,但是Visual Embedding是Random Initialized,所以它的效果自然就很差,其次ViLT虽然说它的推理时间很快,但其实它的训练时间非常非常的慢,我们后面也会提到,在非常标准的一个4 million的set上,ViLT需要64张GPU,而且是32G的GPU训练三天,所以它在训练上的这个复杂度,和训练上的这个时间,丝毫不亚于这个 (c) 类的方法,是有过之而无不及,所以它只是结构上简化了这个多模态学习,但没有真的让这个多模态学习,让所有人都玩得起,所以基于这两个局限性,其实今天我们要讲的第一篇论文,ALBEF就已经呼之欲出了,不过在讲ALBEF之前,我们还是要快速地回顾一下这个CLIP,CLIP模型也是非常简单的一个结构,它是一个典型的双塔模型,就是它有两个Model,一个对应文本 一个对应视觉,然后在训练的时候就是通过对比学习,让这个已有的图像文本对,在空间上拉得更近,然后让不是一个对的图片文本,就拉得尽量更远,从而最后学到了非常好的图像文本特征,然后一旦学到很好的图像文本特征之后,CLIP只需要做这种很简单的点乘,就能去做这种多模态任务,尤其是对那种图像文本匹配,或者图像文本检索的这种任务来说,CLIP简直就是神一样的存在,因为它不光效果好 而且很高效,因为往往你去做这种图像文本匹配,或者图像文本检索任务的时候,你是有一个很大的已有的数据库的,这个时候如果你新来一张图片,或者新来一个文本,你要去跟已有的数据库去做匹配,那其他所有的方法,比如说这里的 (a)、(c)、(d) 都会非常的慢,因为它所有的数据都要过一遍这个编码器,但是CLIP模型就不用,它可以提前把那个数据库里,所有的这个图像文本的特征提前都抽好,而且是想什么时候抽就什么时候抽,抽好放在那儿就行了,等你真正想用的时候,直接就做一个点乘就好了,那矩阵乘法还是相当快的,所以说CLIP的这个实际应用非常的广泛,但是CLIP模型也有它自己的缺陷,它虽然对这种图文匹配的任务非常在行,但是对别的任务,比如说VQA、VR、VE这些任务来说,它的性能就不够好了,因为毕竟光靠一个简单的点乘,还是不能够分析特别复杂的情况,那回顾到这儿,我们现在就来捋一捋,总结一下之前的这些方法,哪些是好的,哪些是不好的,那我们接下来该提出怎样的改进呢,首先我们就来看这个模型的结构,因为我们有图像的输入和文本的输入,刚开始的模型肯定是有两支的,因为它要去抽这个图像文本特征,但是我们一直都在强调,在这个多模态学习里,这个视觉特征远远要大于这个文本特征,所以我们知道,使用这个更大更强的这个视觉模型,比如说一个更大的这个ViT是好的,是我们需要坚持使用的,同时作为这个多模态学习,这个模态之间的融合也是非常关键的,我们也要保证这个模态融合的模型,也要尽可能的大,所以总结完了之后,我们就大概知道,最后如果想做一个很好的这个多模态学习,它的这个网络结构就应该很像这个 (c),也就是说它这个文本编码器,应该比这个图像编码器要小,然而这个多模态融合的这个部分,要尽可能的大,当然这里的这个视觉模型,肯定不想要再用一个这个目标检测模型了,所以更多的是我们会采取一个,比较大的这个Vision Transformer,而不是简单的一个Patch Embedding去做,总之模型结构大概就是长这个样子,那模型有了,接下来该怎么去训练呢?,那之前我们知道,这个CLIP模型就用了一个对比学习的Loss,也就是这个Image Text Contrastive,这个ITC Loss训练,效果就已经很好了,所以我们知道这个ITC Loss应该是不错的,而且训练也很高效,我们应该采纳,那对于之前 (c) 类的这种方法,他们往往因为有这个目标检测,所以他们提出了一个Loss,叫做Word Patch Alignment,就是这个文本的一个单词,和这个图像上的一个Patch,它应该有一个这个对应关系,但是因为现在这个目标检测模型已经没有了,而且在ViLT里头,我们发现这个WPA Loss算起来是非常的慢的,所以才导致这个ViLT模型训练起来这么费劲,所以我们就不太想要这个WPA Loss了,然后剩下的常用的还有两个Loss,一个就是这个我们耳熟能详的,Mask Language Modeling,也就是BERT的训练方式,遮住一个词然后再去预判这个词,完形填空,这个Loss肯定是非常有用,因为到现在为止不光是NLP还是Vision,基本都大一统的全都用Mask Modeling,另外还有一个Image Text Matching的Loss,在之前的 (c) 和 (d) 的这种方法之中,都取得了很好的结果,所以我们也想继续采纳,那我们这轮总结完之后,我们就会发现,可能对于一个好的多模态学习的模型结构来说,我们的目标函数应该也就是,ITC、ITM和MLM这三个的合体,最后的效果应该就不错,那现在如果我们直接跳过来,看ALBEF的论文的模型结构,其实就会发现,我们通过总结做出来的这些预测,基本都是正确的,比如从模型结构来说,在图像这边的编码器,其实就是一个12层的Transformer Base Model,但是在文本这边,它把一个12层的BERT Model,劈成了两个部分,前半部分就是前六层,拿来做这个文本的编码器,后面的这六层,用来去做那个多模态融合的编码器,所以这就满足了我们刚才做的两个假设,第一个假设就是这个图像编码器,会比文本编码器要大,因为这边是12层Transformer Block,这边只有六层Transformer Block,那另外一个就是,模态之间的融合也必须要大,所以这里面并不是一个简单的点乘,而是使用了六层的这个Transformer Block,所以这就跟我们之前做出来的假设,这个模型结构是一模一样的,那另外从这个目标函数上来说,ALBEF就是用了这个Image Text Contrastive,ITC loss,和这个Image Text Matching,和Mask Language Modeling,这三个loss的合体去训练了这个模型,那之所以我们能通过之前的一些对比和总结,得到接下来研究方向的一些假设,而且真的被一些后续的工作所验证,这个其实完全不是巧合,因为大部分工作的这个出发点,或者它的研究动机,其实就是在总结前人工作这个优缺点之中得到的,所以如果大家没有一些研究的idea,或者说不知道接下来这个研究方向,该怎么走的时候,还是需要去阅读更多更相关的文献,而且不光是读这些文献,更重要的是要做一个总结,每一篇论文你都得到了什么样的insight,有什么东西是可取的,可以继续发扬光大,有什么东西是不好的,最好我能把它移除掉的,通过这种不停的阅读、总结、对比,不说百分之百吧,但绝大多数时候,都是应该能够给你一个很明确的研究方向,接下来我们就开始精读今天的第一篇论文,ALBEF,ALBEF叫Align before Fuse,Vision Language Representation Learning,with Momentum Distillation,其实方法的名字就来自于,Align before Fuse,所以ALBEF,作者团队全自来自于Salesforce Research,这个团队在做出ALBEF这个论文之后,又相继做了BLIP、MUST,还有Video那边的ALPro,一系列多模态的工作,质量都很高,所以大家如果有时间都可以去读一读,然后摘要上来就说,最近这个图像文本的这种大规模的特征学习,已经变得非常火爆,因为这个时候CLIP和Align这些工作都已经出来了,那些基于CLIP的后续工作都已经出来很多了,所以说这个方向在21年的时候,真的是非常非常的火,当然到现在还是很火,作者这里说,大部分已有的方法都是用一个Transformer模型,去当作这个多模态的一个编码器,去同时编码这个视觉的Token和这个文本的Token,而这里作者还专门强调,这里的这个视觉Token也就是这个视觉特征,其实就是region-based的这个图像特征,因为在那个时候,大部分之前的这个工作,都还是用这个目标检测器的,但虽然ALBEF跟ViLT,它的这个出发动机都是说,我不想要这个目标检测的模型,但是细节上还是有差异的,ViLT只是说,用了这个目标检测模型以后速度太慢了,我想让它这个推理时间变得更快一些,但是ALBEF的出发的动机就不一样,它是说你用了这个预训练的目标检测器之后,你这边的这个视觉特征,和那边的文本特征其实不是align的,因为你的目标检测器是提前训练好的,然后就只用抽特征,它没有再进行这种end-to-end的训练,所以这就导致你的这个视觉特征,和那边的文本特征可能相隔的很远,这个时候把这两个特征同时扔给,这么一个多模态的编码器之后,有可能这个编码器就不好学,也就作者这里说的,对于这个多模态的编码器来说,可能去学这种图像文本之间的,这种交互信息就会变得很Challenging,那怎么去解决这个问题呢,也就是说如何在这种Multi-Model Encoder之前,你就去把这个图像和文本的特征align起来呢,作者这里就说我们提出了一个对比学习的Loss,就能把这个图像和文本,在Fusing之前就把它们Align上了,所以顾名思义也就是他们论文的题目,也就是ALign BEfore Fuse,那这个Contrastive Loss是什么呢,其实就是CLIP的那个训练Loss,就是图像文本的Contrastive Loss ITC,具体之后我们会讲,所以这个就是论文的第一个贡献,也就是作者认为最重要的贡献,甚至拿这个贡献来命名他们的方法了,那接下来还有什么贡献呢,因为已经有了CLIP和ViLT这一系列的工作,所以大家都知道了,你就用Patch Embedding,就用Vision Transformer就可以了,所以说ALBEF也是这么做的,默认文本,那自然而然,这样ALBEF也就不需要这种Bounding Box Annotation,也就不需要预训练的这个目标检测器,自然输入图像也就不需要是那种,目标检测需要的高分辨率的图像,但这个因为CLIP和ViLT已经有了,所以作者就不过多复述了,更多的是作者要讲下一个他的Contribution,作者这里说,为了能够从这种特别noisy的,这种网上爬下来的数据,去有效地学习这种文本图像特征,所以作者提出了一个方法,叫做Momentum Distillation,也就是这种自训练的方式去学习,自训练你一听,大概率其实就是用Pseudo Label,也就是用伪标签,那伪标签从哪来呢,肯定得是有额外的一个模型,去提供这个伪标签,那在这篇论文里,就是除了已有的模型之外,作者还采用了Moco那篇论文里提出的,这个Momentum Encoder的形式,从而使用这个Momentum Model,去生成这种Pseudo Target,从而达到一个这个自训练的结果,那这个Noisy Web Data到底有多Noisy,它为什么是Noisy的呢,是因为所有这个图像文本对,就是从网上直接爬下来的图像文本对,在很多情况下,它那个文本并没有很好的去描述这个图像,能从网上大规模爬下来的那些文本,其实它有名字叫做Alt Text,也就是Alternative Text,就是另外一种形式的文本,它并不是说,如果你有一幅青山绿水的画,上面有几个人在玩,它就会说有几个人在一个山下面玩,旁边有一个小溪,它是不会这样去写的,因为这样的文本不具备关键词,也就是说对于那些搜索引擎来说,它就不具备价值,对于搜索引擎来说,最具备价值的东西都是关键词,它需要的比如说你喝的饮料,你不能说我在喝饮料,你得说我在喝可口可乐,或者你得说我在喝冰红茶,或者是哪个牌子的冰红茶,这样子当大家去搜的时候,它就会搜到这张图片,同样的道理,你去哪个著名的旅游景点打卡的时候,你也不能说我看到了一座山,也不是搜索引擎想看到的,它想看到的是你去了哪座山,你去了哪个旅游景点,所以这个时候,其实从搜索引擎下下来的这些图片文本对,它里面的文本都是这种具备搜索属性的,但是不是那种特别好的描述性的一个句子,它有时候甚至真的没有去描述,这张图片里是什么内容,它可能就是一些关键词,一些hash tag,所以这里作者为什么想要去解决这个问题,因为你的数据,如果大多数都是这种很noisy的数据对的话,那可能不太能学到一个很好的文本特征表示,而事实上作者不光在ALBEF里这么做,他在后续的BLIP里,他也针对这个问题,还去设计了一个模块叫做caption filtering,这个我们下一期会讲到,去解决这个noisy web data的问题,同样还有很多其他的论文,也针对这个noisy data的问题,提出了很多不同的解决方式,但总之我就是解释一下,这里为什么会有这个noisy web data的问题,其实之前我记得有一篇论文,做了这个数据清洗的工作,他们当时就是把YFCC 100 million那个数据集,给清洗了一下,通过很简单的一些方式,直接就把100 million给筛选到15 million了,就是把5/6的数据都筛掉了,都是noisy的,只有1/6可能质量还比较高,可以用来训练的,所以可见这个noisy的程度是非常高的,好 说完了文章的两个contribution,第一个contribution就是align before fuse,其实就是用了CLIP里的contrastive loss,第二个contribution就是为了克服这个noisy data,提出了一个用momentum model去生成pseudo target,从而做这种自训练的方式,然后作者就是通过这个互信息最大化的这个角度,去做了一些理论分析,结论就是说文章里的这些训练的目标函数,包括LIM、ITM、还有就是momentum distillation,其实它们最终的作用,都是为同一个这个图像文本对,去生成不同的这个视角,其实也就是变相的在做一种data augmentation,从而让最后训练出来的模型,具备Semantic Preserving的功能,就是说只要是语义匹配的这个图像文本对,它就应该被当成是一对,那contribution也说完了,理论分析也做了,最后就是秀一下结果,作者这里说,在这个图文检索的任务上,ALBEF的效果是最厉害的,它比之前的方法,尤其是有一些模型,在这种特别特别大的数据上训练过的模型,其实说的就是CLIP和Align,ALBEF的性能还反超它们,然后在VQA和VR这些任务上,ALBEF也比之前的这个state of art,要绝对的提升了2.37%,还有3.84%的这个准确度,而且这个推理时间也更快了,最后呢,代码其实也开源了,在这个Salesforce下面ALBEF,它们的这个模型,在正规的那个4 million的训练数据集下,能做到一个8卡机,可能是训练三四天的时间,已经是多模态学习里头比较亲民的,或者说最亲民的一个方法了,所以其实我这边很多后续的工作,也全都是基于ALBEF做的,那因为是论文串讲,所以说这个引言部分和相关工作部分,我就直接跳过了,我们直接来看文章的这个主体方法部分,第三章,那3.1和3.2,分别讲了这个模型的结构,以及模型在预训练的时候,用的三个这个loss函数,这些内容,其实我们通通可以通过看图1,都能了解到,那我们先来看模型结构,那其实刚才也大概说过了,那现在我们就讲得更细致一些,那首先我们来看一下图像这边,那图像就是给定任何一张图片,那按照Vision Transformer的做法,我们就把它打成patch,然后通过patch embedding layer,然后送给一个Vision Transformer,那这里其实这就是一个非常标准的,12层的Vision Transformer的base模型,那如果这里的图片是224x224的话,那我们之前也说过,那它这里的这个sequence length就是196,然后加上额外的一个这个CLS token,那就是197,然后它的这个特征维度就是768,所以说这里面这个绿黄色的这个特征,就是197乘以768,但是在这篇论文里,作者也说了在预训练阶段,他们用的这个图片是256x256,所以说这里这个绿色的这个sequence length,就会相应的再长一些,那总之这个Vision Transformer的模型,是没有什么可讲的,就是非常标准的,而且它的这个预训练参数,其实也就是用的DEiT,也就是Data Efficient Vision Transformer,那篇论文里,在ImageNet 1K数据集上,训练出来的一个初始化参数,那文本这边呢其实就有点意思,那按道理来说,如果你先把ALBEF这个模型,看成是一个CLIP模型的话,它其实就是左边一个VIT,右边一个BERT model,那如果你在这上,直接去做对比学习的话,它就变成CLIP了,但是我们刚开始在介绍的时候也说过,这个多模态的任务,它就必须得有一个多模态的,这个融合的一个过程,如果你只是单纯的有一个图像模型,然后还有一个文本模型的话,没有这个多模态特征的融合,它做这些VQA VR任务的时候,效果就不会很好,那有人可能就会说,那我就在这个一个Vision Transformer,和一个BERT model上面,我再加一个这个multi-model编码器,不就行了吗,当然也是可以这么做的,但这样你的计算量就进一步加大了,而且我们之前也提到过,说这个视觉模型,是应该比这个文本模型要大一些的,这样子它在多模态的这些任务上,效果才会好,所以综合这两方面考虑,最简单直白的一种方式,那就是说我把文本这边的,这一个大模型给劈开,劈成两个部分,我只用前六层去做这个文本编码,因为本来我就希望这个文本编码器,比这个图像编码器要弱一些,然后把剩下的那六层transformer encoder,就直接当成这个multi-model fusion的过程,这样一举两得,跟之前的方法保持同样的计算复杂度,但是我满足了之前总结出来的这个最优配比,那至于文本模型这边,其实它就是用一个BERT模型去做的这个初始化,然后它中间的这个特征维度也是768,然后它也有这么一个CLS token,代表了整个句子的这个文本信息,那到这呢,就把ALBEF的基本的模型结构讲完了,当然了,ALBEF这边为了去做后面的这个,momentum distillation,而且为了给这里的ITC loss提供更多这个negative,所以说它还有一个momentum的model,也就是说除了这个ViT和这个BERT,这一份模型参数之外,它还有这边对应的有一个ViT和BERT,另外的一份模型参数,而这边的这个模型参数,是由这边在训练的模型参数,通过moving average得到的,也就是这个momentum model,这个就跟MoCo是一模一样的,这里通过把moving average的那个参数设的非常高,在论文里是0.995,从而来保证这边的这个momentum model,不会那么快的更新,所以说它产生的特征就更加的稳定,不仅可以拿来做更稳定的这个negative sample,而且还可以去做这种momentum distillation,这个具体我们会再回头在momentum distillation里提到,总之整个模型结构就是长这个样子了,那接下来我们就来说一下这个目标函数,首先我们就来说一下最开始遇到的这个ITC loss,说到对比学习我们之前就知道,只要你能定义一个正样本对,然后定义很多负样本对,然后我们就可以去对比了,我们希望这个正样本对之间的距离越来越近,希望正负样本对之间的距离越来越远,那首先我们要做的就是去抽取这种全局特征,然后在这个特征之间,去做这种embedding space上的这种拉近和拉远,那在ALBEF里首先一个图像i,它通过这个vision transformer之后,就会得到这些的特征,那这里我们把这个黄色这个CLS token,也就当做它的这个全局特征,也就是一个768×1的一个向量,那文本这边也一样,先是tokenize之后,就把这个文本text变成一个tokenization的序列,然后我们把它扔给一个BERT的前六层,得到了一系列的特征,这边的CLS token,也就当做了这个文本的全局特征,是一个768×1的向量,那接下来的操作跟MoCo是一模一样的,首先我们先做一下downsample和normalization,就把这个768×1变成了256×1的一个向量,同样的这个文本这边也是768变成256×1,那一旦你有了这个正样本的两个特征,就是这两个特征,你就希望它尽可能的近,那它的负样本全都存在一个q里,这个q里有65536个这个负样本,就很大很大的一个q,但因为它没有gradient,因为它是由这个momentum model产生的,所以说它并不占很多内存,那这样因为你有很多很多的这个负样本,然后你通过这种正负样本之间的这个对比,你就可以去对这个模型进行第一阶段的学习了,这其实也就是作者在这篇文章中说的,align before fuse的align,也就是说在我们把这个图像特征,和这个文本特征,扔到这个Multi-model Fusion的encoder之前,我就已经通过这个ITC loss,这个对比学习的loss,让这个图像特征和文本特征尽可能的拉近了,尽可能的在同一个embedding space里了,那因为这里的ITC loss的这个实现,是完全按照Moco来的,就是算了一个cross entropy loss,所以这里我就不过多复述了,那有了这个ITC loss之后,其实这个Vision transformer的12层,和这个BERT的前6层其实都可以训练了,但是呢BERT的这个后6层,也就是这个多模态的这个融合部分,该怎么训练呢,那我们就先来说一下这个ITM loss,也就是Image Text Matching,Image Text Matching其实很简单,就是说你给定一个图片,给定一个文本,然后这个图像文本,通过这个ALBEF的模型之后,就会出来一个特征,那在这个特征之后加一个分类头,也就是一个FC层,然后我去判断,到底这个I和T是不是一个对,那说白了,这个ITM就是一个二分类任务,那这个loss虽然听起来很合理,我们确实应该用它,但是实际操作的时候,你会发现这个loss太简单了,因为判断正样本可能还有点难度,但是判断谁和谁是负样本,这个就太简单了,因为如果你不对这个负样本做什么要求,那基本上很多很多的这个图片文本,它都可以当成是,现在图像文本对的负样本,所以这个分类任务,很快它的准确度就提升得很高很高,那在预训练的时候,训练再久其实也没有任何意义了,那这个时候一个常见的做法,就是说我在选这个负样本的时候,我给它一些constraint,那在ALBEF这篇论文里,它就是采取了最常用的一个方法,就是通过某种方式,去选择最难的那个负样本,也就是最接近于正样本的那个负样本,具体来说在ALBEF这篇论文里,它的这个batch size是512,那对于ITM这个loss来说,它的这个正样本对就是512个,那对于这个mini batch里的每一张图像,我去哪找它的这个hard negative的文本呢,这个时候ITM还依赖于之前的这个ITC,它就把这张图片和同一个batch里,所有的这个文本都算一遍这个cos similarity,然后它在这里选择一个除了它自己之外,相似度最高的那个文本当做这个negative,也就是说其实这个文本和这个图像,已经非常非常相似了,它基本都可以拿来当正样本用,但是我非说它是一个负样本,也就是hard negative的定义嘛,那这个时候呢ITM loss就变得非常challenging了,然后让这个模型更好的去判断,谁到底是一个图像文本对,也就是它这里说的image text matching,那最后一个目标函数呢,就是我们耳熟能详的Mask Language Modeling,BERT里用的完形填空,它其实就是把原来完整的句子,这个text T变成一个T',也就是说有些单词被mask掉了,然后它把这个缺失的句子和这个图片,一起通过这个ALBEF的模型,然后最后呢,去把之前的这个完整的句子给预测出来,那在这里呢,其实它不是像NLP那边单纯的一个MLM了,它其实也借助了图像这边的信息,去帮助它更好的恢复这个,那个单词被mask掉了,但这里呢有一个小细节很值得关注,就是说在我们算这个ITC loss,和这个ITM loss的时候呢,其实我们的输入都是原始的i和t,原始的i和t,但是呢当我们算这个MLM loss的时候呢,它的输入是原始的i,但是是mask后的t,这意味着什么呢,这说明ALBEF这个模型,每一个训练的iteration,其实它做了两次模型的forward,一次模型的forward呢,是用了这个原始的i和t,另一次模型的forward呢,是用了原始的i和mask的t,当然了不光是ALBEF这篇论文,会有这种多次前项的这个过程,其实ViLT,包括之前的很多模型,它都会做好几次前项,甚至做三次前项过程,这也是其中一个原因,就是为什么多模态学习普遍的方法,它的训练时间都比较长,因为它为了算好几个不同的loss,它也得做好几次不同的forward,去满足各种各样的条件,那到这呢3.1和3.2节就讲完了,最后我们可以看到,在文章的这个公式5,这个ALBEF的所有在训练时候,用的这个目标函数呢,就都在这里了,也就是itc、mlm和itm的这个合体,那说完了模型结构和所有的这个目标函数,接下来我们就来说一下,这篇文章的另一个贡献,也就是这个momentum distillation,动量蒸馏,那作者上来就先说,它为什么要做这个动量蒸馏,它的这个动机是什么呢,主要就是这个noisy的web data,这个其实我们在讲摘要的时候也说过了,就是从网上爬下来的这些,正的这个图像文本对,其实它经常都是这个weakly correlated,就它们之间的关联不是那么强,有的时候甚至都不匹配,经常的情况就是说它里面的那个文本,有的时候会包含很多单词,就是说其实跟这个图像没什么关系,或者说这个图像里也包含很多的物体,但是在这个文本里并没有得到体现,那这个时候,这种noisy的data会造成什么影响呢,那主要就是在算这个目标函数的时候,这个有时候就会有偏差,比如说我们在算这个ITC,就是对比学习的loss的时候,对于一张图片来说,它的这个所谓的负样本文本,其实很有可能也描述了这个图像里的很多内容,它可能不是爬下来的那个,Ground Truth one hot的那个image text pair,但是其实这个文本,可能已经很好的描述了这个图像,甚至可能比Ground Truth描述的还好,但是我们非要把它当成是一个负样本,那这个时候就会对ITC的学习造成很大的影响,那另外对于这个MLM loss来说,完形填空,那其实我们做过那么多年的完形填空也知道,那有很多时候这个空里是可以填很多单词的,有的时候是会存在这种比Ground Truth,还要描述这个图片更好,或者说差不多好的这个文字出现,所以综合这些考量,作者最后的结论就是说,你用这种one hot label,就是你从网上爬下来的,就是这个图片和这个文本就是一对,其他跟它都不是一对的这种one hot label,对于ITC和MLM这两个loss来说是不好的,因为有的负样本也包含了很多很多的信息,一味的在算loss的时候去惩罚这些负样本,其实会让模型的学习非常的困难,那既然这个问题是由这个Noisy Data里的,one hot label带来的,那很直接的一个想法就是说,我如果能找到额外的这个监督信号,最好它不是one hot,它是multi hot,或者说它就是另外一个模型的这个输出,不就好了吗,那这个时候这两年比较火的一种方式,叫做self training,就是自训练就很自然的,能够应用到这个场景里来解决这个问题,从最开始Google的Noisy Student,在ImageNet上把分刷的那么高,还有到最近自监督的DINO,它其实也算是一种自训练模式,所以说作者这里也就采取了这种方式,就是先构建一个momentum model,然后用这个动量模型去生成这种pseudo targets,这个pseudo targets其实就是一个softmax score,它就不再是一个one hot label了,具体这个动量模型是怎么构建的呢,其实就是在已有的模型之上,去做这种exponential moving average EMA,这个技术其实是很成熟的,现在大部分的代码库里都是支持这个EMA的,包括DEiT, Swin Transformer,都是自带EMA的,它的目的就是说在这个模型训练的时候,我们希望在训练原始的这个model的时候,我们不光是让它的这个预测,跟这个ground truth的one hot label去尽可能的接近,我们还想让它这个预测,跟这个动量模型出来的这个pseudo targets去尽可能的match,这样就能达到一个比较好的折中点,就是说很多的信息我们从one hot label里去学,但是当这个one hot label是错误的,或者是noisy的时候,我们希望这个稳定的momentum model,能够提供一些改进,具体我们现在拿ITC loss做个例子来说的话,原来我们的ITC loss就是这个L itc,但是因为它是基于这个one hot label的,所以这个时候我们想希望再算一个pseudo target loss,去弥补它的一些缺陷和不足,后面的这个loss跟前面这个loss,也就是equation1里的ITC loss的不同,就把这个ground truth换成这个q,就是这个pseudo targets,因为现在q不再是一个one hot label,而是一个softmax score,所以这里面我们算这个KL divergence,而不是cross entropy,最后因为我们有两个loss,一个是原来的这个ITC,一个是现在的基于pseudo target的ITC,所以说我们分别给它1减α和α的loss weight,最终就得到我们这个momentum版本的ITC loss,同样的道理对于这个MLM loss来说,它也是用这个新生成的这个pseudo target,去代替了原来的ground truth,所以在原始的这个MLM loss之外,我们现在又有一个新的,基于这个自训练的这个MLM loss,所以最终ALBEF的这个训练loss其实有五个,就是有两个ITC,两个MLM和一个ITM,这个时候大家可能会想,为什么ITM loss我们不也给它一个自训练,给它一个这个动量模型的版本呢,是因为ITM这个loss,本身它就是基于ground truth,它必须要知道你是不是一个pair,它就是一个二分类任务,而且在ITM里我们又做了hard negative,这跟momentum model其实又有conflict,所以说ITM并没有动量的这个版本,讲完了这两个新的loss,作者还在这个图二里给了一些这个例子,其实就是为了证明给你看,就是这些pseudo target,其实是比ground truth那个image text pair要更好的,能起到更好的这个监督训练作用,那么现在来看图二,首先上面这一行主要是来讲这个MLM loss,下面是讲ITC loss,我们可以看到对于这个图片来说,如果我们把文本里的,就polar bear in the什么地方,这个给mask掉,它的那个ground truth是说in the wild,就是在野外的一个北极熊,但其实由这个动量模型,产生的top five的pseudo targets来说,其实有的时候更准确,比如说在动物园或者在游泳池里,在水里在池塘里,这些应该是比ground truth的wild更具备描述性的,尤其是你拿这个图像文本对来学习模型的话,其实后面的这些pool water pond的这些,应该是比这个wild要好很多的,因为它确实描述了这个图片里的这个水,后面的这两个例子也差不多的情况,比如说最后一个例子,它说在这个森林里有一个瀑布,ground truth是说有一个很远的瀑布,但是你从描述这个图片本身出发,其实后面的这些pseudo targets应该是更好的,比如说一个小瀑布或者一个美丽的瀑布,或者这种隐约的瀑布,其实都远比remote这个单词要描述的更形象,这个是对于完形填空,就是说这个空里其实有很多别的单词,是比这个ground truth要表现更好的,对于ITC loss来说,当你有一个图片的时候,你有一个对应的ground truth的文本,而下面那些就是从整个数据集里,去挑出来的别的这种pseudo targets,然后我们会发现其实这个ground truth,说有一辆车在路上抛锚了,并不能很好的描述这张图片,因为这张图片里的主体还有另外一个人,但是所有的这些pseudo targets都很好的描述出来了,有一个年轻的女人,有一个女人,所以他都把这个主体人给描述出来了,而且这里面也都有车,而且甚至还把这个树也描述出来了,所以这就再次验证了我们之前说的那个问题,就说从网上爬下来的这些图像文本对,很多时候这个文本是并不能描述这个图片的,那作者这里的图2的这些例子就举的是非常好,一下就让你看到了使用这个pseudo target的好处,那到这其实文章的这个主体方法部分就讲完了,两个contribution,一个是align before fuse,另外一个是momentum distillation,接下来我们就来快速过一下,文章预训练时候用的数据集,和这个下游任务的一些任务描述和数据集,以及最后的一些实验结果,那接下来我们就看这个3.4,在预训练的时候使用的数据集,这个还是比较标准,作者上来就说我们是follow之前的UNITER,他们一共用了四个数据集来预训练,分别是Conceptual Captions,SBU Captions,COCO和Visual Genome,前面两个数据集都比较大,这个CC其实有两个版本,一个是CC 3 million,一个是CC 12 million,在这里他说的是3 million,所以就已经300万了,这个SBU Captions其实就是1 million,然后COCO和Visual Genome分别都只有10万,所以加起来是有400多万张图片,但这里面有两个点值得说一下,第一个点,就是虽然COCO和Visual Genome,是原来的作者就已经提供了这个数据,所以你永远都可以下到原始版本的数据,谁都不会缺少,但是对于前面这两个,CC 3 million和SBU Captions,这两个数据集来说,原来的论文作者只是提供了一个URL的List,你如果想用这个数据集,你得自己去下,这就会导致一个数据缺失的问题,随着这么多年过去了,其实我们自己在下的时候,像这个CC 3 million,我们就只能下到2.3 million的图片了,就是少了70万个图片,像这个SBU Captions,大家能下到的也就只有80多万,也不到1 million了,所以说这里面也比较tricky,就是你预训练的时候用的数据,应该和别人都不一样,每个人和每个人都不一样,但因为这个数据集本身也比较noisy,所以你有的时候差个几万个图像文本对,对最后的这个训练影响也不是很大,但如果你差个几十万,那其实一般还是会有一些影响的,所以以后大家如果想做什么任务,看到一个什么新的数据集,不管你做不做,如果感兴趣的话,你可以先把这个数据集下着,免得以后真想做这个数据集的时候,因为缺失数据,所以总比不过别人的分,第二个想强调的点,就是说对于前面这两个数据集来说,它都是一个图片对应一个文本,但是对于后面这两个数据集来说,它其实是一个图片对应好几个文本,比如说像COCO这里,一张图片就对应了五个文本,所以也就是作者这句话说的意思,就是说这个不一样的图片,一共有四百多万,这个是对的,但是其实这个图像文本对有五百多万,那多出来的这一百多万的图像文本对,其实就是因为COCO和Visual Genome,每个图片它都有额外的文本所导致的,然后最后作者还做了一个实验,因为ALBEF的一个非常大的contribution,就是说这个momentum distillation,也就是说处理这个数据集里的,这个noisy的问题,那如果你扩大这个数据集,就是用更noisy更大的一个数据集去训练的时候,是不是就能获得更大的成效呢,那在这篇论文里确实如此,当他们使用了这个CC12 million的时候,就是第五个数据集的时候,他们把这个数据集,总共就扩大到14 million,1400万这么大,就跟ImageNet 21K那么大,这个时候,他们在各个下游任务上的性能又猛涨了一波,总之在ALBEF这篇论文里,它的这个预训练数据集就有两个setting,非常标准,一个就是这个4 million的setting,就是有四个数据集,一个就是这个14 million的setting,就是把这个CC12 million也加进去了,这两个setting是最常被用的,包括之前19年20年,一直到现在22年都是比较常用的,当然了,现在这个预训练数据集规模也越来越大,大家很多人已经不满足于使用,就是只有14 million这么多图像文本对了,开源的LAION已经有了400 million,而且已经有了2 billion 5 billion这么大的数据集,所以以后多模态的预训练方面,可能不是一般人能玩得动的了,说完了预训练时候用的数据集,接下来我们就看一下,下游的Vision Language的任务,ALBEF这篇论文做的还是比较全面的,前后做了五个任务,第一个就是图文检索,当然这个里面包含了图像到文本的检索,还有文本到图像的检索,其实在现实应用中还有文本到文本的,其实也就是搜索,还有图像到图像的以图搜图,总之这个问题是非常关键的,商用价值也非常的高,但具体问题很好理解,就是说给定一个数据库,我们怎么去搜到Ground Truth的图像文本对,因为是检索,所以我们衡量的指标就是这个Recall 召回,一般用的是R1,R5,R10,也就是说在你检索回来的一个,五个或者十个Sample里,里面有没有这个Ground Truth Sample,如果有就算你找到了,接下来几个任务分别是Visual Entailment,也就是视觉蕴含,它其实就是说给定一个假设,一个前提,我能不能去推理出这个前提,如果能推理出来,我就说是一个蕴含的关系,也就是这个Entailment,如果说前后矛盾推不出来,就是这个Contradictory,如果没什么关系,你也不知道推得出来还是推不出来,那就是中立,就是Neutral,所以其实一般情况下,大部分工作都把这个Visual Entailment,变成了一个三分类的问题,因为你是分类问题,所以很自然你的衡量的指标,就是分类准确度,那至于VQA任务来说,大家应该也是耳熟能详了,就是视觉问答,顾名思义就是给定一个问题,然后给定一个图片,你看你能不能回答这个问题,提供一个Answer,然后VQA其实很有意思,它一般有两个Setting,一个就是也看作是一个分类问题,它的这些答案都是固定的,就是就那么一个Set,我从里面去选,那这样就变成了一个,Multi-Answer Classification的问题,比如说对于VQA2.0这个数据集来说,它就有一个提前设定好的3192个Answer,这个一般就被称作叫做闭集VQA,就因为你这个Answer的Set,是个闭合的集合,然后另外一边相应的肯定就是开集VQA,意思就是说反正你要去生成一个答案,也就是说你要生成一个文本,所以你是一个文本生成的任务,也就是说你是需要一个Transformer Decoder,去生成这个Answer,那这个开集VQA它的任务难度就大了很多,因为你有可能生成了正确的答案,或者说很相似的答案,但是它跟Ground Truth不一致,这个时候还是会被判错,但ALBEF这篇论文虽然说他们是做了,这个Answer Generation的问题,但是他们在做推理的时候,还是把这个生成的答案,限制到了那3192个答案里,所以严格意义上来说,它也不算那么的开集,问题还是稍微简化了一些,而且也还是一个分类问题,所以说衡量指标也还是准确度,第四个任务就是这个Visual Reasoning,视觉推理,这个任务就是去预测一个文本,能不能同时描述一对图片,所以它是一个二分类任务问题,当然这个衡量指标也是准确度,然后最后一个任务是这个Visual Grounding,但其实Visual Grounding属于它自己的一个领域,很多多模态表征学习的论文里,都不会去涉及这个Visual Grounding的任务,都是专门做Grounding的论文,会去刷这些Visual Grounding的数据集,所以这里我们就不过多复述了,最后终于到了看结果的时候,作者首先是做了一个消融实验,来验证他们文章里提出来的这么多东西,到底哪个有用哪个没用,消融实验一般也是我最爱看的部分,因为可以得到很多有用的insight,首先作者把这个,用了MLM和ITM training loss的这个,当做了baseline,因为基本上所有的之前的工作,和现在的工作里面都会有这两个loss,所以这就像一个出发点一样,既然所有人都有,我就可以把它当做一个基线模型去对比,首先作者就给上面加了这个ITC,也就是Align Before Fuse里的Align,然后我们就会发现这个提升是非常巨大的,基本这里是两个多点,三个点,然后三个点,两个点,两个点的提升,而且是在这么多任务上,这个检索、VE、VR、VQA,四个任务上都有明显的提升,所以这个ITC loss真的是YYDS,也就是说CLIP、MoCo,这种对比学习的方式还是很厉害,即使是mask modeling当前非常火爆的情况下,我觉得这个对比学习还是有值得挖掘的点,或者值得有继续做下去的潜力的,接下来的一个小贡献,就是作者在ITM里提出的Hard Negative,这个我们可以看到,大概都有0.5左右的提升,虽然看起来不是那么显著,但是毕竟所有任务上都有提升,所以也是相当不错的一个技巧,这个也是意料之中,毕竟在对比学习出来之后,有很多很多篇论文都去研究Hard Negative,对对比学习的影响,而且Hard Negative这个概念在很早之前就有,而且应用到了各个方方面面的领域,甚至是目标检测或者物体分割,里面也都有Hard Negative的应用,最后我们就来看一下这个Momentum Distillation,也就是这里这个简写MoD,其实我们可以看到,如果给ITC上加这个MoD,大概就是0.3左右的提升,然后给MLM上加了MoD,就只有再有额外的可能0.1左右的提升,然后最后是在downstream的,就是下游任务上再去做MoD,然后大概会有0.3 0.1 0.2左右的提升,总之如果是在预训练阶段的MoD来看的话,它总共的提升可能也只有0.3 0.4个点,甚至还比不过Hard Negative,所以相对于使用ITC loss或者Hard Negative,或者使用更大的数据集,所有的这些因素来讲,这个Momentum Distillation带来的提升不是那么大,但是这个研究方向还是很好的,怎么从Noise Data里去学习有效的表征,我觉得也是一个非常有趣的研究方向,接下来我们可以再快速看一下,在每个具体任务上ALBEF的表现,首先这个表2表3都是做的图文检索,我们先从表3开始说,就是这个Zero-shot的图文检索,Zero-shot其实就是跟CLIP模型一样,就我先预训练好一个模型,我接下来就直接抽特征,然后去算Cosine Similarity,我就可以直接检索走起,我是不需要在下游任务上去做fine-tune的,但因为COCO这个数据集,已经在预训练的时候被使用过了,不论它有没有用COCO里的这个标签,严格意义上来讲,我都不能再在COCO上去做Zero-shot了,所以说作者这里只是在Flickr30K上,去做了这个Zero-shot,然后作者这里说,你看我们这个ALBEF多么的强,我们只在4M的这个数据集上去训练,然后这个Zero-Shot的结果,就比之前这个CLIP和Align,在400M或者1.2B上训练的模型的结果还要好,但其实这里面有一个小点需要注意的,就是其实ALBEF这里说的Zero-shot,只是针对于Flickr的Zero-shot,它其实在预训练完之后,还在COCO那个数据集上,又做了一遍有标签,就是有监督的fine-tune,但是CLIP和Align其实都是直接做Zero-shot,所以这也算是方法各显神通吧,毕竟这个CLIP和Align,这个预训练的数据集太大了,那反过头来,我们看这个fine-tune的Setting,也就是说在Flickr上去做fine-tune,或者COCO上去做fine-tune,ALBEF也是比之前的这个Align,还有之前的这些UNITER、OSCAR,这些state-of-art的模型都要好不少,尤其是我们可以发现,当用了这个更大的数据集,就更多的这个Noisy Data之后,可能Momentum Distillation还是起到了应有的作用,所以这个的性能提升非常的显著,另外一个小点其实要说的,就是Flickr30K这个数据集,其实也已经被刷爆了,我们可以看到这都已经到100了,而且COCO也已经非常的高了,所以说图文检索领域,其实是需要新的数据集了,有可能是一个更大的数据集,也有可能是一个Annotated更好的数据集,或者是一个视频文本的这个检索数据集,总之这也算是一个可以填的坑,那接下来我们一起看表4,作者其实就把VQV、VR、VE,这三个任务全都放到一个表格里了,因为它们都变成了一个分类问题,它们都用准确度来衡量,所以说很容易就放到一个表格里显示就好了,同样我们可以看到,跟之前最强的这个OSCAR比,那这个ALBEF 4M其实就已经比它都要好了,或者跟我们之前讲过最新的这个ViLT比,那就更不用说了,ViLT的性能其实是比较差的,它只是说它的推理时间比较快而已,尤其当ALBEF上了14M这个训练数据集的时候,这个性能在三个数据集上都是非常之强的,当然ALBEF这篇论文还有很多别的消融实验,还有很多别的细节,还有一些非常不错的可视化,但这里我们就不一一复述了,感兴趣的同学可以自己去看,总之 ALBEF不论是在训练速度上,还是在推理速度上,还是在通用性或者在性能表现上,都是非常的亮眼,而且它还及时开源了它的代码,所以真的算是去年,多模态学习领域里一个承上启下的工作了,虽然说是论文串讲,但因为有太多需要介绍的这个Setting,数据集、下游任务,还有一些常见的这个预训练目标函数,所以说ALBEF还是跟精读一样,读了40分钟,那接下来我们来看21年,另外一篇比较火的论文叫做VLMo,作者团队全部来自于这个微软,这个团队近几年,真的是出了很多大名鼎鼎的工作,比如说BEiT V1、V2、V3,还有LayoutLM V1、V2、V3,还有做语音的、做视频的,真的是多模态领域里非常solid的一个组,所以大家如果想做多模态学习,任何一个领域的多模态学习,都可以去看一下他们组发的论文,应该或多或少都会有一些联系的,那这篇论文的贡献其实有两点,一个就是模型结构上的改进,也就是他这里说的Mixture-of-Modality-Experts,另外一个就是训练方式上的改进,他们做的这种分阶段的模型预训练,这两个改进其实都师出有名,都有非常强的研究动机,接下来我们就直接去引言看一下,这两个研究动机,在引言的第二段,作者上来就说了第一个研究动机,就是他们为什么要介绍这个Mixture of Experts,作者说现在在这个多模态学习领域,大概有两个主流的模型结构,一个就是像CLIP Align这种的,他们采取了一个dual-encoder,也就是双塔结构,图形有一个模型,文本有一个模型,双塔完全分开的,谁跟谁都不染,然后模态之间的交互,就是被一个非常简单的Cosine Similarity去做的,它的好处我们上次也说过,非常明显,尤其是对这种检索任务来说极其有效,因为它可以提前把那些特征都抽好,然后接下来直接算Similarity就好了,矩阵乘法还不是飞快,所以说极其适合这种大规模的图像文本的检索,非常具有商业价值,但是它的缺点也由此而体现,就是说如此Shallow的交互,也就是说只算了一个Cosine Similarity,是无法做这种多模态之间,非常难的各种情形的,比如说在ViLT这篇论文里,ViLT的作者就发现,即使CLIP那么的强,但是CLIP其实在一系列的下游任务上,比如说VR,它其实就比不过之前的state-of-art的方式,自然肯定就有另外一系列的工作,你之前是双塔,现在肯定就是单塔,单塔就是Fusion Encoder的方式,就是我先把图像和文本分开处理一下下,但是当做模态交互的时候,我用一个Transformer Encoder,去好好的做一下模态之间的交互,这样就弥补了你之前这个双塔模式的缺陷,所以说在这个Visual Language Classification的Task上,也就是我们刚才说的VR、VE、VQA,取得了这个Superior Performance,就是效果特别好,但是它也有问题,就是当你去做检索任务的时候,又出麻烦了,因为你只有一个模型,你必须同时做这个推理,所以当你这个图像文本对特别多,数据集特别大的时候,你就要把所有all possible这个图像文本对,全都要同时的去编码,然后去算这个Similarity Score,而你才能去做这个检索,所以说它的这个推理时间就会非常非常的慢,所以对于大规模数据集来说,去做检索的话基本就不太现实了,那鉴于这种情况,一个很直接的想法就是说,既然你各有各的优缺点,那我能不能把你放到同一个框架里去呢,然后在做推理的时候,我想把你当做这个dual-encoder来用,我就把你当dual-encoder来用,我想把你当Fusion Encoder来用,我就把你当Fusion Encoder来用,那如果能达到这个灵活性,那岂不是特别美好,所以说作者这里就引出了,他们这篇文章提出的,这个Mixture-of-Modality-Expert,具体的细节我们会接下来照着图讲,但简单来说就是这个自注意力,所有的模态都是共享的,但是在这个Feed Forward FC层,每个模态就会对应自己不同的Expert,就是视觉就有视觉的Vision Expert,Language就Language的Expert,Multi-model就Multi-model对应的Expert,这样在训练的时候,哪个模态的数据来了,我就训练哪个模态的Expert,然后在推理的时候,我也能根据现在输入的数据,去决定我到底该使用什么样的模型结构,这样就非常优雅的解决了,第一个这个研究难题,另外一个研究动机,就是在引言的第四段,作者上来说VLMo其实在它训练的时候,它的这个目标函数,也是ITC、ITM和MLM,所以跟ALBEF是一样的,所以在这篇论文里,我都不需要再过多复述了,但这样就会有一个让大家很感兴趣的问题,就是这个训练数据的问题,因为我们都看到了,在NLP那边用了Transformer,随着这个数据的增加,这个结果就会不停地变好变好再变好,在视觉这边虽然暂时没有看到,就是这么好的这个Scaling的性能,但是对于多模态来讲,因为它里面也有文本,所以说做多模态学习的人,他也希望说看到,当你这个训练数据集越多的时候,你的这个模型的性能就越好,CLIP其实已经在某种程度上,验证了这一点了,所以大家自然是会想,在更多的数据集上去做预训练的,但可惜在当时,就是ALBEF和VLMo的时候,Lion团队还没有推出LAION 400 million,或者LAION 5 billion这样开源的数据集,CLIP用的那个WIT数据集也并没有开源,所以说对于研究者来说,他们自己如果想去构造,这么大规模的一个数据集来说,这个effort是非常大的,这个时候一条曲线救国的道路,很自然的就摆在面前,那就是说,虽然多模态的训练数据集不够,比如说只有4 million的setting,或者14 million的setting,但是在单个的modality里,就是视觉或者NLP里,有大把大把的数据可以去用,即使你是想有监督的训练,视觉里也有ImageNet 22K,有14 million的数据,就已经比多模态这边最大的,14 million的setting还要大,那如果你是说我想要无监督的预训练,那可用的数据更是多的数不胜数,那文本那边也是多的数不胜数,所以说基于这个研究动机,本文VLMo的作者就提出了一个,stagewise pre-training strategy,就是说我分阶段去训练,既然你这个视觉和NLP领域,都有自己各自的这么大的数据集,那我就先把vision expert,在视觉数据集这边训好,然后我再去把language expert,在language那边的数据集上,text-only data上训好,这个时候这个模型本身这个参数,已经是非常好的被初始化过了,这个时候你再在多模态的数据上,去做一下pre-training,效果应该就会好很多,而事实也确实如此,这个stagewise pre-training strategy,给VLMo带来了很大的提升,接下来我们就先看一下,VLMo的模型结构长什么样,然后快速看一下它的结果,那我们直接来看文章的图1,首先我们来看一下图1的左边,也就是VLMo这篇论文的一个核心,它也是一个transformer encoder的结构,但是它在每个transformer block里面,做了一些改动,也就是他们提出的这个MoME transformer,mixture-of-modality-expert,具体来说其实我们都知道,一个标准的transformer block里面,就是先有一个Layer Norm,然后有一个MSA multi-head self-attention,然后再Layer Norm,然后再跟一个FFN feed-forward network,然后最后有一个residual,然后这个就是一个标准的transformer block了,然后这里我们可以看到,这个layer norm、MSA、layer norm,还有这个residual connection这些全都是一样的,唯一一个不一样,就是在这个feed-forward network这块,它不是一个feed-forward network,而是针对不同的这个输入,不同的modality,它有这个vision FFN、language FFN,和这个vision language FFN,也就是它这里说的这个switching modality expert,从而构建出了它的这个MoME transformer block,然后最后构建出了VLMo整个的模型结构,这里面其实一个比较有意思的点,就是说虽然后面这个FFN层,它没有share weights,它是各自modality有各自的这个FFN层,但是之前的这个self-attention层,是完全share weights的,也就是说不论你是图像信号,还是这个文本信号,还是图像文本信号,你任何的这个token sequence进来,我的这个self-attention这个model weights,全都是一样的,通通都是share weights的,所以这也就是我为什么觉得,transformer这个结构很好,或者说多模态学习接下来会是一个趋势,因为这个多模态学习搭配上这个transformer,真的是一个绝佳的组合,transformer这个自注意力操作,它真的是用了最少的这个inductive bias,所以它不挑输入,基本上我们现在已经看到了很多的这个证据,就是说同样的这个self-attention weights,它可以用来做不同的这个图像文本音频视频,很多这样的任务,你是不需要重新去训练这个自注意力参数的,接下来在VLMo这篇论文,这个分阶段训练里面,我们还可以看到更强的这个evidence,更加的有意思,这里其实在介绍完MoME这个transformer block之后,就没有什么可讲的了,VLMo论文像我刚才说的一样,它也是用了Image Text Contrastive ITC,还有Image Text Matching ITM,和这个Mask Language Modeling MLM,这三个loss去训练的模型,而且它还从ALBEF里借鉴了,这个hard negative mining的思想,所以说它也是用ITC去做了更好的这个ITM,所以说训练loss是完全一致的,那至于它是怎么去算这个loss,它的这个fusion encoder到底长什么样呢,其实跟ALBEF也差不多,只不过更灵活了一些,比如说当我们去算这个ITC,contrastive loss的时候,VLMo直接就化身为了这个CLIP模型,就是图像这边就单独只有图像的输入,然后进去了一个ViT,它里面的FFN都用的是Vision FFN,这个L呢,如果你用的是这个Vision Transformer Base,那就是12层的一个transformer,那文本这边就是文本的token,单独进去这个language model,后面用的是language expert,这个就是一个12层的BERT base,所以说如果你只看这个ITC这块呢,它就是一个CLIP模型,然后当我们去看这个ITM,或者说这个mask language modeling的时候,它又化身成了fusion encoder的形式,就是说这个图像和文本的这个输入,一起进去,一起进这个multi-head self-attention,但这里的这个self-attention,跟之前的self-attention,self-attention和后面的self-attention之间,通通都是share weights的,都是一样的,不管你是什么modality,自注意力的参数都是不变的,都是share,然后在前面的L-F层,就在L-F的transformer block里,它是对这个视觉和language信号,分别去做模型的,所以这就是分别去用这个Vision Expert,和这个Language Expert,只有在最后的这F层,它才去用了这个Vision Language Expert,在论文的最后的实现细节里,作者说,如果你用的是一个transformer base模型,这块其实前十层,后面这个F就是2,也就是说后面只有两层transformer block,去做这个模态之间的融合,然后ITM就是一个二分类任务,然后Mask Language Modeling,就是去预测这些被mask掉的单词,那我们看完整套的这个模型结构,和训练方式之后,我们就会发现,VLMo这篇论文的好处,它就是灵活,训练的时候,你可以通过这个各种modality,你可以去选择,我到底训练这中间,哪个modality expert,然后在做推理的时候,你也可以去做选择,因为所有的模型参数都在那里了,如果你想做这个检索任务,你就像CLIP一样,就用这个两个模型就可以了,如果你想做这些Vision Language,这些分类任务,VR、VE、VQA,那你就用下面的这个,Fusion Encoder模式就可以了,但是它的灵活也不是白来的,那就像我们在ALBEF里说的一样,因为它有的时候用这个mask的输入,有的时候不用这个mask输入,所以ALBEF里就要做两次forward,而在VLMo里面,它应该也是至少做了两次,甚至三次的这个前向过程,作者在后面说,VLMo这个base模型,在4 million的setting下训练,用64张V100的卡也要训练两天,所以说又回到ViLT那个级别的训练量了,比ALBEF要慢,但总之这个模型结构,真的是很灵活,也很有趣,所以作者团队在接下来的时间中,还在继续打磨而且使用它,比如说今年最新的这个BEiT v3里,它还是使用了这个MoME Transformer结构,去做BEiT v3,讲完了模型结构上的改进,我们就来说一下,第二个文章的contribution,就是这个分阶段的训练策略,因为作者想利用Unimodality里,那些大量的这些图片文本,去做这种很好的预训练,提供一个更好的模型初始化,所以说作者就先去做了这个Vision Pre-training,然后去做Language Pre-training,然后最后才去做这个Vision Language Pre-training,然后在做Vision Pre-training的时候,肯定是Unsupervised,他们就用了他们自己团队提出的这个BEIT,就是Mask Image Modeling,然后在做Language Modeling的时候,就是Mask Language Modeling,最后Vision Language Pre-training,就是我们刚才说的那三个目标函数,但是这里面特别有趣的一个点,就是作者在训练的过程中,到底哪些层是冻住的,哪些层是不冻住的,我们接下来来仔细看一下,作者这里说,蓝色的这个虚线就代表是Frozen FFN,然后这个橘黄色的虚线,代表Frozen Self-attention,那么看到在做第一阶段的,这个Vision Pre-training的时候,因为你是刚开始训练完,你肯定没有什么需要freeze的,因为所有的东西都是随机初始化了,所以说这12层的这个Transformer Block,也就是包括前面这个自注意力,然后和后面的这个Vision Expert,都是打开训练的,但是当你到第二阶段,去做这个文本的预训练的时候,我们会看到这个Vision Expert被冻住了,因为你现在是文本数据嘛,你不需要去训练那个Vision Expert,所以Vision Expert的那个,FFN层参数就固定下来了,我是要去训练这个Language Expert的,但是非常有意思的事,是它把这个Self-Attention给冻住了,意思就是说,我完全拿一个在视觉数据上,训练好的这么一个模型,在视觉Token Sequence上,训练好的一个自注意力模型,我可以直接拿来,对这个文本数据进行建模,我都不需要fine-tune,它这个Self-Attention就工作得很好,我就能让这个模型,一样能把这个完形填空做得很好,当然作者好像也是,之前也有一些工作也是了,就是如果我反过来行不行,就是我先在Language上去训练,然后再在Vision上冻住去做,好像结果不太好,但是如果是先用Vision训练,然后再在Text上直接去用这个Self-Attention,已经在很多工作里证明是有效的,这个对我而言很奇怪也很有趣,如果对这个现象感兴趣的同学,也可以继续去深挖一下,看看是不是真的所有的modality,都能用同样的这个Self-Attention,而且是为什么,那到了第三阶段,因为这时候做的,就是我们想要的这个多模态了,所以说该打开的就全打开了,不光是这个Self-Attention,还是后面的这三个Expert,就都打开去做fine-tune了,这个就是本文所讲的第二个Contribution,分阶段的预训练策略,讲完了方法部分,我们快速的看一下,VLMo的这个实验结果,在表1里作者就对比了,这个VQA和VR这两个数据集上的表现,这里因为VLMo,主要做的是模型结构上的改进,所以VLMo这里是,既做了这个Base也做了Large,但是它的预训练数据集都是4M、4M,但是ALBEF就不一样,ALBEF因为是想克服这个,训练数据集的Noisiness,所以说ALBEF只做了Base,但是它做了4M和14M,所以也就是说,根据你这个论文不同的这个Contribution,根据你的这个关注点在哪里,其实你也不是说,所有的这个Setting都要去比,或者所有的Setting都要去做的,你要选择那些对你有利,或者说能更好的把你的故事,讲出来的这个Setting去做,反而会让整个文章读起来更顺,而且更有说服力,当然了,其实我记得第一版的时候,这个VLMo是没有这个部分的,它没有去做这个VLMo Large++,也就是说在这个1.0B数据集上,去训练的结果,可能是最近,反正也做了BEIT v3,这个VLMo也又训练了一波,所以就把更好的结果更新到Arxiv上了,总之呢,VLMo还是非常有效的,它在4M上这个数据集上的表现,就已经非常亮眼了,它跟ALBEF去做这种公平对比的时候,是比ALBEF的全线都要高的,要两到三个点,所以算是significant improvement,然后如果用了更大的模型,或者甚至在更大的数据集上,去做预训练完之后,这个性能的提升就更不用说了,鉴于BEIT v3也使用了MoME,这种模型结构,而且它又这么灵活,所以大家可以好好感受一下,如何用很小的改动,就是把一个FFN做成多个Expert,但是就带来了这么巨大的提升,这样可能也是很多经验积累出来的,VLMo这篇论文,其实实验做的也是非常的详尽,也有很多的消融实验,比如说去证明,它这个分阶段的预训练策略很有效,还有它这个模型很灵活,所以它在这个单独的视觉数据集上,也取得了很好的效果,还要去做这种图文检索的时候,也取得了很好的效果和推理时间,这里我们就不一一复述了,我更想说一下的是,VLMo这篇论文的结语部分,作者说了很多,就是说在未来,我们会做很多继续去提升VLMo的方式,然后作者团队是真的做到了,而且一一都deliver,比如说第一个最直白的,就是直接去scale,就是把这个模型变大,这个在作者接下来的BEIT v3里就实现了,它就用了ViT-Giant 有1.9 billion的参数,第二个就是做更多的下游的Vision-Language Task,比如说其中有一个更著名的,Image Captioning 就是图像字幕,做Captioning一般是需要一个Transformer Decoder,所以我们这一期讲了,这个ViLT, CLIP, ALBEF和VLMo,都不太适合能去做,接下来作者在VL-BEIT、BEIT v3里,都去做了尝试,第三点作者想说的就是Unimodality,能够帮助Multimodality,同样的Multimodality也有可能能帮助Unimodality,同样的在BEIT v3的工作里,作者刷的不光是多模态的数据集,他把文本和图像的各个数据集也全刷了一遍,效果都非常好,最后一个更宏观的目标,就是说我不光是想做Vision Language,我肯定有更多的模态和更多的应用场景,比如说Speech、Video或者Structured Knowledge,其实作者团队也做了很多这方面的工作,比如说在Speech这边就有WAVLM,在Structured Knowledge这边就有Layout LM v1、v2、v3,还有就是去支持这种General Purpose的多模态学习,这个最近也是比较火,就是统一用这个文本当做一个Interface,这样所有的任务都可以通过一个Prompt,然后去生成文本这种结构去实现,作者团队这边也出了一个MetaLM的工作,所以算是一步一步把他们之前提出的Future Work,全都实现了,这个还是非常难能可贵的,这里我更想说的,其实就是做研究是一点一点积累上来的,作者团队做了这么多有影响力的工作,其实也是一步一步迭代出来的,比如说我们来看一下BEIT这一系列的工作,它的发展历程,这个BEIT V1在2021年6月份就出来了,接下来在2021年11月份的时候,就出了VLMo这篇论文,因为这个时候,图像也可以用Mask Modeling去做,文本也可以用Mask Modeling去做,所以很自然的到22年6月份的时候,作者团队就推出了VL-BEIT,就是同时用Mask Modeling去做Vision Language,接下来又过了两个月,这个BEIT V2就出来了,BEIT V2其实是BEIT V1的一个升级版,它还做的是视觉这边的Dataset,而不是做的Multimodality,同样的月份,22年8月份又出了BEIT V3,BEIT V3其实就是之前,所有这些工作的一个集大成版本,就是一个多模态的网络,但是同时它也做了Unimodality,所以就是在这一步一步的积累过程之中,才能做出来这么多solid的工作,在多模态的下期串讲之中,我主要就会讲一些最近的,基于这个Transformer Encoder Decoder的工作,来看一下多模态学习,到今年为止它的发展又到了哪个阶段