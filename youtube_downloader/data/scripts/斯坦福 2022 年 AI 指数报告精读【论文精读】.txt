【制作人：奔崩】,一些天前啊,斯坦福发布了2022年的 AI 指数报告,这篇230页的报告里面,很好的概括了 ai 在过去一年里面的一些进展,今天我们就带大家读一下这篇报告,从一个比较大的视角里面,给大家看一下 AI 各个领域的一些技术进步,因为这篇报告没有特别深的技术门槛,所以大家读起来是相对比较容易的,首先我们看一下这个报告是谁写的,他来自于斯坦福下面一个叫做 HAI 的一个机构,他的全称叫做以人为中心的人工智能,他应该是一个虚拟的机构啊,是由李飞飞老师和另外斯坦福一位做逻辑的老师,一起共同建立的,这个机构可能最有名的事情就是,他从2017年开始呢,每年发布一个 AI 的指数报告,到这一期是第5期,他2020年可能因为疫情的缘故他错过了,我们知道其实每一年有大量的这种机构,特别是咨询公司,他会去发布各种人工智能的报告,那为什么我们选的这一篇来讲呢,是因为我觉得这篇报告,覆盖的面还是比较广的,前面说到他有230页,他其实可以认为是将来这几个不同机构的这样子的报告,把他合并起来,并且整理成一个完整的报告,所以我们就读他一个,就能得到一个比较全面的了解,好接下来我们来看一下正文,有意思的是他在正文的第一页呢,画的就是重点,因为作者也知道这230页的文章,估计大家不会从头看到尾,所以呢 他在一开始的时候就给大家说,这里一共有8个重点,你实在不想看下去的话,你看完这8个重点也就行了,我们来看一下 这8个重点到底在说什么,第一点是说在 AI 的投资上面,在过去的一年有非常大的一个增长,在2021年一共有将近1,000亿美金的投资,然后他比2020年啊基本上是翻倍了,但是这些投资更加集中了,是因为2021年一共只有746家,新成立的公司,但是在2020年的有762家,而且在2019年更多有1,000多家,所以因为你公司更少了,你的投资更多了,所以每个公司得到的钱也就更多了,所以这个也比较好理解啊,在 AI 的前面几年大家都觉得啊,这里是一块蓝海,大家赶紧过来圈钱,所以大家纷纷的去成立公司,过去的几年里,大家也看到一些公司发展不那么顺利,所以大家从中也得到了很多经验,所以在今天你再去成立公司的话,你会更加小心,同样投资人也会看的更加谨慎一点,但是对一旦看中了项目,因为有过去成功的经验,投资人可能信心也就更足了,而且一些几年前成立的公司啊,到现在在拿投资的话,他拿到了钱当然就更多了,因为每一轮投资,通常要比上一轮投资有所增加,所以这个地方,他说在2020年只有4轮投资,他是超过了 5个亿美金,但是在2021年呢这里一共有15轮投资,里面每一个是超过了5亿美金的,第二点是比较有意思的是说,中美的合作,是在跨国家合作里面最多的,他也提到是说,中美关系在过去一年里面啊,也是比较紧张,但是呢中美科学家在共同发论文,是在所有跨国家合作里面是最多的,而且其中的第二名是中国和英国的科学家合作发论文,但是中美科学家一共发表论文是在中英他们的合作的2.7倍,虽然大家说科学家是有国界,而且技术可能也是有国界的,因为一个国家可以封锁另外一个国家的技术,但是在科研本身来讲,是整个人类的共同的财产,因为科研只是告诉你说这条路能走通,但是呢你要把沿着这条路走下去,最后做出工作,能够服务于各个国家的人民,还是有大量的技术落地的事情,但是科研的本身啊,应该是尽量的是公开的,因为这样子才能够,促进我们发现更多东西,毕竟对于整个人工智能来讲,我们离我们的终极目标,就是能够模拟人类一样的思考和行为,还差的很远呢,第三个是说我们的语言模型,现在能力更强了,但是呢也更加的偏见了,我们之前讲论文的时候是有讲过,我们现在模型也越做越大,比如说 gpt 3能做到的,1,000亿的模型的规模,而且能够做出很惊讶的一些效果,但是他又在这里提到说,相比于2018年,就是 BERT 这篇文章,用的是一个亿的参数来讲,2021年训练出来的有2,800亿参数的模型呢,但是他生成出来的那些带有偏见的有毒的结果增加了29%,这是因为大模型使用更大的数据,这些数据不像小数据那么能够精心给你准备,而且大模型 更容易的把大数据里面那些,带有偏见的一些东西,给你捞出来,所以导致大模型其实危害性更大一些,第四点是讲 AI 的伦理,他说在过去几年里面,有关 AI 伦理的文章其实越来越多,我们在讲 OpenAI,还是 DeepMind的几篇工作时候,他们在最后都提到过些伦理,就是把这个模型应用到生产中的时候,可能会碰到什么样的公平性啊,公开性的一些问题,然后第五点是说 AI 变得越来越便宜,而且性能越来越高,他是我们训练一个图片分类器的开销,从2018年到现在相比的话,我们下降了63.6%,而且在训练的时间呢,我们缩短了94.4% 啊,这是因为我们现在 gpu 做的越来越大,而现在 gpu 做大的时候啊,如果你不去算挖矿导致的 gpu 价格升高的话,其实他的还是成本还是在往下降的,而且我们现在能够用更大的集群,用更好的算法能做分布式的训练,所以他的整体计算时间是往下,第六点是关于数据,他说在我们这个报告里面的10个测试数据集上面,其中有9个方法,就最好的方法用了额外的数据,我们在论文精读系列里面也讲过,很多方法能够利用没有标号的数据,来提升自己的性能,然而第七点是说,在全球范围里面,关于 AI 的立法也是越来越多了,然后最后一点讲的是说,你的机械臂变得更便宜了,然后在2017年的时候,平均一个机械臂的成本是4万美金,现在我们,基本上只要一半的价格,就是2万美金能买到一个机械臂了,所以这个就是这个报告想给大家表达的8个重点,首先我们怎么理解这些重点,怎么把它串起来呢,我们就得想一想我们一个技术,他的一个发展的生命周期是什么样的,大家图示一下整个技术的一个发展曲线,假设蓝色这个线是我们的技术的发展曲线的话,你的 x 轴是你的时间,从技术一开始到后面,然后你的 y 轴是你的成熟度的话,你可认为一个技术,它其实是有三个阶段的,第一个阶段是说这个技术刚刚出现,就是你出现突破性的一个技术,然后在这个地方是一个快速发展,就是技术做的更大更便宜,然后效果更好,然后再到第三个阶段,是你要个技术,需要在更多地方做应用的时候,你要做到更安全,所以一个技术是有三个阶段,一个是说刚刚冒出来,第二个是说做大做便宜,第三个是说做安全,那么我们回过头来看,其中列的这个八点,你可认为做出来这个事情啊,其实都没有提到,比如说什么是做出来呢,AlexNet出来是出来吧,然后GAN那篇文章,以及说2018年的 BERT 这篇文章,分别是这几个领域的突破性的工作,但是在2021年,我们似乎没有看到在某个领域有一个,突破性的工作,也就是说我们关心的这些应用里面,他的突破性工作已经在之前完成了,当然有可能今年或者未来几年,有新的一些突破性工作的出来,说你没有突破性工作的话,那么就表示你这个周期已经过了,那么进入你的快速发展的周期,什么叫快速发展就是数据更大,模型更大,然后你的训练成本更低,效果更好,所以这里提到的数据啊,你的训练的成本啊,然后你的机械臂的成本啊,都是属于这一块,第三个是说,你一个技术 真的要在,实际生产中被广泛使用的话,那么大家会关注他的安全性,所以这个地方我们讨论伦理啊,大的语言模型里面的偏见,以及一些立法上的东西,所以这个地方你可认为我们这8个点,基本上是对应,在这个技术的这两个阶段,所以你可认为深度学习的技术呀,至少在我们这篇文章关心的一些领域上,基本上是在这一块的阶段呢,在画完重点之后我们来看一下目录,这篇文章呢一共有五个部分,其中第一个部分讲的是整个学术圈,也就是发表文章的一些情况,第二个呢,讲的是在 AI 的各个应用里面,新的模型,他在我们核心的标准数据集上,他的一些性能的一些发展,第三个讲的是技术的伦理性,第四个呢讲的是整个就业的市场啊,以及包括我们的学生的数量,第五个呢讲的是政府的一些立法相关,好我们下面就是一章章给大家读下来,第一章关心的是论文呢专利的一些情况,首先第一张图讲的是过去的11年里面,ai 相关的论文的个数,这个报告有意思的地方在于,他的核心内容就是那么几百张图,所以呢,我们就把这个图啊逐一看下来就是了,这也是为什么我之前说的,这篇文章的技术含量并不高,所以阅读门槛是比较低的,首先看一下这个图啊啊,这个是2010年,然后可以看到在2017年,特别是2018年开始,整个的论文的趋势是一个,虽然还是一个线性的关系,但是长得比较多,在过去早期几年里面长得还是行的,然后在这个地方有一个明显的一个增的趋势,特别是在去年,整个 AI 相关的文章发表了30多万篇,你像我们这个论文精读系列,一年能够讲个50多,或者有时候我们一期讲几篇论文啊,可能那么讲个70篇文章就了不起了,但是这个地方,其实我们文章里面一共有30万篇,这个是非常吓人的,我觉得不会有人能够真的读完那么多,一般来说,就算你是一个一线的研究者来,你一年可能读个几百篇就差不多了,那么你可能会说,这个领域发表那么多文章干什么,其实绝大部分99.9%的文章可能是,没有太多意义的,这一点我们在之前读论文的时候,也多多少少提到过,其实真正的能够影响到很多人的文章,是非常非常少的,也就是说我们每一周读一篇文章,其实只要你能坚持下去,其实也能够把,整个领域中的文章给你读完,虽然你一年发表30万篇,但我一年能够给你读50篇,其实也是非常好的一个数字了,然后在50篇文章以外的,比如说这几千篇文章吧,其实你的主要作用是写给那么,几个人看,或者十几个人看,你能启发这些人,在你的基础上再往前走一点点,所以你还是有一定意义的,但是对绝大部分,剩下的那几十万篇文章,很多时候真的就是一个,训练的一个手段,就是可以认为是一个,大家学 AI 的一个练习题,就是你上什么课你得做题对不对,你得做个项目,那么呢,你学 AI 的时候你怎么样学最好呢,做自己做的研究然后把文章写出来,这是一个非常好的练习题,而且我们整个领域啊,也就整个科研领域,都是通过导师指导学生做项目,写文章来培养学生,所以这个里面,你可以认为大量的是一个,同学们的练习题,但是因为看到论文的大量的增长,所以我们也能知道,亲近到这个领域的同学是越来越多了,第二张图显示的是过去11年里面,然后不同文章这些类别,首先有意思的是,你看到这个蓝色的线是杂志的文章,在过去几年里面,杂志的文章是一飞冲天啊,然后排名第二的是会议的文章,可以看到虽然我们有增长,但是因为从2019年开始呢,但是因为2019年开始有疫情的缘故呢,他是往下走的,然后是一个叫做 Repository 的东西,其实你可以认为就是啊,Arxiv 上面的文章呢,所以这个比较有意思的是说,你会发现杂志是最多的,其次是会议,然后才是你的 Arxiv 文章,所以我们可能印象是一个,我们可能会觉得说啊 Arxiv 文章是最多的,然后你发表在好的会议上的文章是少的,让你去投杂志的是更少,但实际过程中是反过来的,是因为我们其实关注的都是一些顶级的会议啊,顶级的杂志他们当然是小的,但是还有大量的,不那么顶级的会议,和不那么顶级的杂志,所以虽然我们看到在过去几年,Arxiv 的文章确实是上升的比较快,但是呢那些会议和杂志增长的更多,这是因为现在各个机构啊,对于Arxiv 的文章还是不那么承认,因为他没有经过同行审议,但是杂志文章和会议文章,是经过同行审议的,他可以算成你的科研的成果啊,因为现在大量的钱进来了,然后大量的老师啊同学进来,所以有发论文的压力,所以导致了大量的杂志和会议就冒出来了,而且相对来说啊,杂志比较比会议能赚钱一些,所以呢杂志更多一点,因为你投杂志的时候,你是要交版面费的,十年前他可能还要打印一本杂志寄给你,现在都是电子版,所以他往上面一放就行了,所以他是一个百赚不赔的买卖啊,会议呢相对来说,你投了个文章之后你还得去开会,会议方还得租一个场馆啊,把你组织各种活动啊,所以呢会议赚的钱没有杂志多,这为什么,看到杂志啊在过去真的是涨势非常迅猛了,接下来显示的是论文他的标签的一个汇总啊,可以看到最多的是一个叫做模式识别的一个标签,我们其实没有特别讲过模式识别啊,我们都是讲的机器学习,机器学习是排的第二在这个地方,你可认为,机器学习和模式识别的区别在于是,模式识别讲的是一个任务,就是说我要去在数据里面,识别某一个东西出来,就是说比如说我要去做图片识别啊,做目标的检测,都是一个模式识别的任务,机器学习是其中的一个技术,我们可以使用,机器学习这个技术来解决模式识别,我们可以使用别的技术吗,机器学习跟别的地方不一样的是说,机器学习是说,我从数据里面训练一个模型,再去数据里面发现我的规律,可以看到是机器学习啊,比计算机视觉啊别的都要长的话,应该可能是他把深度学习这一块是放在,机器学习里面,因为我们知道,机器学习在过去五年里面,也是深度学习这一块是发展的特别大的,所以导致这一个突飞猛进,下面看到的是计算机视觉啊,数据挖掘啊,以及自然语言处理这个就是各个领域了,所以他们跟机器学习的关系,还是说机器学习是一个技术,但是下面这个都是一些任务,你可以用机器学习的方法来做计算机视觉,你可以不用机器学习的方法做计算机视觉,但是因为机器学习,或者说深度学习在,过去五年里面的发展的迅猛,所以很多时候,我们也把机器学习或者深度学习作为别的,这一些应用啊,或者技术的一个代名词了,接下来展示的是说,每篇文章的作者,他都是来自于什么样的机构,他说你是来自于大学就是教育机构,还是来自于,这个是不知道啊,还是来自非盈利机构或者来自公司,全球范围的话就是59%是来自于学校,然后5%是来自于公司,然后这个地方非盈利组织,我还真不知道他到底指的是谁,比如说 open ai,他一开始说是非盈利组织,现在好像他也变成了盈利组织了,所以他也不算啊,所以到底是谁是非盈利组织呢,总不能是红十字会吧,下面还有几张类似的图,他是按国家来分的,在美国的话,可以看到,学校其实占的比例是往下走的,但是公司再过去几年,在科研上的投入是有所上升的,但是比较有意思的是说在中国,来自于学校的作者是在往上增的,我觉得这应该是来自两块吧,第一块是说 AI 方向学是那个扩招,第二块是说有更多的老师能力更强,能够指导学生来写论文呢,下面这个图展示的是中美科学家合作的论文的绝对的数量,可以看到,在2020年的时候是达到了一个顶峰,到了一年发了1万篇的样子,但是在2021年有所下降但仍然是非常多 有9,600篇,因为中美合作论文数字远远大于别的一些国家的合作,所以他用单独一张图给大家显示,非中美合作的文章,可以看到排名第二的是,中英的一些合作,第三的是美国和英国的合作,排名第四的是中国和澳大利亚的,所以看到是跟美国合作的包括了,美英啊美中,在2021年他的论文数上都有下降,所以我们在之前讲到,中美合作在2021年下降的时候,可能不一定,是因为中美关系不好造成的,很有可能就是美国自己,在合作上面的政策有所变化,下面这个图表示的是这些杂志的文章,主要是来自于哪些国家,可以看到主要是来自于中国,这有可能是,国内高校的考核里面,更偏重杂志的文章吧,下面一个图展示的是所有的杂志文章,按国家划分他的总引用数,排名第一的仍然是中国,但是注意到,中国在杂志上的发表其实比第二,在论文数量上是多1.5倍的,但是在引用上呢,你好像总数没有高于1.5倍,所以导致呢,这一块我们的,论文的引用数不见得比别人要高,接下来是会议文章的统计 但是按国家分,可以看到中国,在发表的会议文章上面,也是有一个下降趋势,之后然后迅速上升,也是排在第一的样子,所以他比美国啊将近多了两倍的样子,但是从引用数上来说,美国发表的文章,他的引用数是中国的两倍,也就是说,所以就是说平均一篇会议的文章,中国文章的引用数,比美国文章的引用数要少四倍,我们知道引用代表的是一篇文章的影响力,有点像 b 站视频的点赞,或者是投币一样,但反过来讲,你不能说中国发表的会议文章,他的技术含量或者他的质量,是要比美国发表的低四倍,你的引用其实来自于很多因素,包括你文章本身质量怎么样,第二个是说你写的怎么样,在写作上当然我们是要吃亏的,就是你如果写英文的话,你可能不如,直接讲英文的人写的那么好,如果你不写英文的话,那你的整个受众要变少,你的引用数也会变低,还有一个说你整个研究院的名声吧,因为这一块啊,相对来说还是,欧美这一块积累的比较多一点,中国研究者在这一块也是新晋的势力,所以完成一个名声的积累还需要一些年,所以这些因素加起来,导致你的引用数会偏低,这张图表示的是发在Arxiv 上面,来自不同国家的文章的一些分布,可以看到美国是持平的,但是呢欧洲是下降的,中国是有个显著的上升,我觉得这是一个好事情,就是说你把文章发在Arxiv 上面,意味着是说,你更关心的是你文章的影响力,和你的跟别的科学家进行交流,而不是说我一定要投到一个杂志先,投到一个会议先,先等等审稿这样子,我说接受了之后我可以去回去拿奖金,所以这一点,也可以认为下面这个图有证实啊,就是这个是发在Arxiv 上面,文章的引用数,可以看到虽然中国是比美国英国要低,但是呢你的发表的文章数要低,所以你除下了,你会发现每篇文章的平均的引用数,其中美差距是比较少的,在Arxiv 上面至少,下面这个图呢,展示的是 AI 相关的一些专利,他蓝色的线表示的是你生了多少个专利,然后你绿色的线表示的是多少个专利给予了批准,如果大家没有投过专利的话,其实这个东西有点像发论文,通常是律师帮你来写,然后投到专利局,专利局进行审核,然后告诉你通过还是不通过,有点像论文的评审啊,所以可以看到,虽然我们国家在专利,投稿数是一路飙升啊,超过了所有的国家,但是在接收率上面还是略低于欧洲,而且远远的少于美国,就说明说,我们在专利的投稿质量上面呢,还是有待增强吧,接下来是说在过去11年里面,比较大的那些 AI 的会议,他的人数,然后最早是2011年的,可以看到当时候最大的会是 CVPR,有2,000个人参加,那一年的 CVPR 在旧金山召开,其实我也去了,到时候感慨说,做计算机视觉的人就是多,然后同年召开的 icml 其实比 CVPR 少很多,我记得应该是不到1,000人的样子吧,当然现在在回忆啊,11年前去 CVPR 的时候听过什么报告,基本上不太记得了,但是记得比较清楚的一点呢,倒是一些八卦的消息,比如说听李飞飞老师讲,说他对一个研究工作他关心什么,他关心的是说你能讲好一个故事,他关心的是,你是不是能把你的实验的结果能够,直观的显示出来,让大家有直观上的一些了解,而不是说我告诉你精度到了某个点,另外还听了很多八卦了,比如说哪个教授和另外哪个教授在一起了,哪个教授分手了,哪个教授去一个学校面试然后没有去另外一个,大佬觉得很伤心,或者哪个老师对学生不好了,这个八卦信息倒是听了很多了,但是后来没想到是 icml,其实我一直觉得是一个比较小的一个会议,在过去的几年里面突飞猛进,在然后在去年的时候,参会的人数竟然达到了将近3万人,而且反超了一直在过去今年排名,第一的 NeurlPS,其实对我来讲,我还更喜欢去小一点的会,这样子的话,你能跟你熟悉那几个人,做比较深入的了解,而而且你会更专注的去听一下他的报告,当你的会变大的时候,基本上就是看一个热闹吧,就是那今天晚上去哪个公司的社交晚宴呢,明天去哪个公司社交晚宴呢,你拿没拿这个公司的,送的一些小玩具啊,你你有没有看某个特别 Fancy 的 demo 啊,就是整个人的注意力就很分散了,好这就是第一章,讲的是杂志会议和专利的一些情况,但他最后还有一点点的结果,但我觉得那个数据不是很充分,我们就不讲了,我们接下来看一下第二章,第二章呢讲的是技术上的进展,应该是这个报告最核心的部分呢,我们首先来看一下他的目录,可以看到啊,他其实也是按照你的领域做了划分,他先讲的是计算机视觉里面,跟图片相关,接下来讲的是计算机视觉里面跟你的,视频相关的部分,然后呢就跳到了自然语言处理,然后是推荐系统,强化学习,硬件以及最下的机器人,然后在每一章里面你可以看,到他其实是按照子任务,然后对每一个子任务呢,他把你的核心的数据集给你拿出来,然后在每一章里面,讲的是各个新的算法,在这些数据集上的表现,如果你读过 paper with code 的话,其实你可以认为他们两个的,组织结构是非常相似的,paper with code 可能在数据集上更多一点,而且模型上也更多一点,因为毕竟模型是可以用户上传的,但是这个报告啊整理的更好一点,在阅读上更加方便一点,我们先来看第一个任务,计算机视觉里面的图片,图片分类啊这个大家应该都很熟悉了,他这里画的是 CIFFA 10,这个数据集的一个示例图,就是每张图片里面去识别他的物体,到底是谁,然后他当然举例的是 ImageNet,这个数据集了,我们在这个数据集上也讲过很多次啊,在论文精读系列里面,下面这个图呢展示的是按照年份开始,就是2012年 alex net 才出来的时候,在 ImageNet 这个数据上的,top 1的精度的一个变化,Top1的精度就是讲你在模型,预测最准的那个标号,是不是跟真实的标号是对应上的,这个蓝色的线表示的是,只用 ImageNet 这个数据集上的精度,可以看到他一直是在上升的,有意思的是这个绿色的线,他表示的是,如果你允许使用额外的数据集的话,会怎么样,我们在之前讲过几遍啊,使用额外的数据集,大的没有标号的数据集来帮助你提升,可以看到从2018年开始使用,额外的数据集能够在他的基础上,做到更好一些,这个是,而且这个趋势还是在往上涨的,接下来讲的是 top five 的一个精度,就是说你的模型给5个标号出来,只要其中有一个标号跟我的,真实标号是一样的,那就是表示你分类对了,所以这个当然就比较容易点,可以看到区域上差不多,但是呢精度会更高一点,就是说如果你使用额外的数据集,按top five 的精度的话,现在可以达到99%了已经,之所以讲 top five,是因为这里有个人类的进入,人类大概能做到95%的程度啊,所以其实在2017年的时候,基本上计算机就已经超过了人类,在这个数据集上面,那么大家会问说,既然你已经超过人类了,而且已经到99%了,还有没有往前做了必要,对这个问题我的观念是两点,第一个是呢,在 ImageNet 上,继续往前做的必要也不那么大,就是说如果你为了把 ImageNet 的精度,刷的很高,其实意义不大了,因为你 top five 已经99%了,你 top 1也到了90%,然后你继续往上提升呢,你的空间也就那么大了,但是对于整个计算机视觉的图片识别来讲,我们其实还是有一点差距的,为什么呢是因为 ImageNet这个数据集啊,他是1,000类的数据集,他每一类他有5,000张图片,意味着是说,你整个图片的质量是不错的,而且呢你对每个类别有足够多的图片,但是在实际应用中,因为这个世界上物体的个数,你肯定至少是百万或者千万级别吧,第二个是对每个物体,你很难收集个几千张图片,所以在实际任务中,我们对于大量的没有充分多的数据,他的识别,以及在各个情况下比如说晚上的识别,各种不同的光照,各种不同的角度的识别,其实我们还是有很大的距离的,在这一块上我们还在往前做,但是大家为什么一直在刷 ImageNet 这个数据集呢,我觉得原因有三点吧,第一点是当然他很有名,就是说你不报告一下 ImageNet 上的精度,大家会觉得很奇怪,第二呢好像,没有一个比 ImageNet 好很多的数据让你来测试,第三呢大家发现是说,ImageNet 这数据还是够大的,所以一个模型在上面表现的很好,比如说模型 a 比模型 b 要好,可能在实际应用中,在别的任务上面,模型 a 很有可能,也还是比模型 b 要好的,所以就是说,他也算是一个比较,靠谱的评测数据集吧,在图片分类之后呢,他讲的是图片生成这个应用,我们在讲GAN呢是有提到过,我们可以用这种生成网络来合成一张图片,这个地方显示的是,从2014就是GAN这篇文章开始,合成人脸上的一些进展,从2014年是黑白的然后比较模糊的,到了前年和去年,基本上可以生成高清的人脸的大图,而且从人的角度来看,确实是,很难看出来这个是真人还是假人呐,在各种细节上啊,表情上啊,甚至大家可以去做一些,样式的一些定制,可以看到是在这一块进展也是非常大的,从量化的角度,我们如何来衡量这个进展呢,通常我们用的是一个叫做 fid 的一个分数,这个在2020年开始啊,在GAN相关的文章里面被广泛的使用,这个分数呢,他衡量的是说,生成的图片,和你真实的图片之间的一个区别,然后他里面主要有两点啊,第一点是怎么样衡量你的关系,他的做法是说把,整个生成的图片和你,真实的图片呢,看成是一个多变量的一个高斯分布,然后来衡量这两个分布之间的区别,然后这个 inception 的意思呢是说,他把生成的图片和真实的图片,都放到一个 inception v 3的一个网络里面,然后把他的网络的,中间的那些输出拿出来,把这中间输出,当做是一个高斯随机变量,然后去看他这两个之间的区别,然后把他合起来之后就得到一个分数,下面这个图呢显示的是在2018年开始啊,到去年在 stl 10这个数据上的一些进展,可以看到也是一直在下降,而且在降到了这个地方,因为这个分数衡量的是一个距离,所以是越小越好,所以在下降就意味着我们一直有进展,当然这是一个定量的分析,但是图片生成这个呢,就大家一般会去更定性的去看,说到底跟真实的图片差别什么样,主要是一个人的一个感知吧,在图片生成之后,他讲的是一个叫做 deep fake检测的一个任务,就是我们在讲GAN的时候讲过啊,在GAN出来之后,其实最有名的英文叫做 deep fake,就是把一个人的人脸,换到另外一个人的脸上,这个应用在出来之后被大量的广告啊,成人啊以及说虚假信息里面使用,比如说他举了一个例子,是用 deep fake 这个技术呢,造假了一个奥巴马来喷川普的一个视频,这个视频在网络上有200万以上的点击,然后造成了很大的一个误解,然后随后也很多政府出台了法律,是说禁止 deep fake 在用在,一些特定的一些领域,然后在这个应用上有几个数据集啊,一个叫做人脸取证++的数据集,他的做法是,在 youtube 上找了1,000个原始的,一些视频,然后呢用 Deep Fake技术来生成一类,负类的视频,然后你要去判断,你一个视频到底是正类,是原始的还是被假装出来的,然后这个是2019年的数据集啊,在今天来看,这几个算法基本上达的精度还不错啊,基本上可以达到99%的样子,然后在另外一个数据集上叫做,名人数据集,他在 youtube 上找了590个名人的一些视频,然后呢,再用 Deepfake生成了5,639个假的数据集,在这个数据集上,大家的精度就没那么高了,就是这是 auc 啊,只到了76.88%,所以表示我们在这一块判断,其实还是没那么准确的,接下来的一个任务是人的姿态的估计,在这个任务里面呢,你要干的事情是说把一个人啊,他些关键点给你找出来,比如说这个地方,你看到他把人的眼睛啊,脖子然后枝干各种手啊脚的位置给你找出来,一般来说你找的是一些关节点,然后找出来能干什么事情呢,他这举了几个例子,比如说你能去做体育的一些分析,比如说你可以去分析一个运动员,他的动作,然后去看你们有没有改进的可能性,特别是对新手啊,你可以通过,对他的一些姿态的一个监控,来看你是不是比较标准,这一点在运动界,其实现在是用的比较广泛的,第二个是他讲的是一个人的监控啊,后面的是一些虚拟人的一些设计,目前主流的一个办法,去采集虚拟人的动作,是在人的演员的关节上贴很多传感器,然后这些人呢去做动作,然后把他的整个记录下来,虽然他的精度非常高,但是他是非常贵的一件事情,因为你得贴很多东西,然后你有场地的一些限制,使用一个纯基于视觉的方案的话,那么你的成本就会变得非常低了,但还举了一个例子是说在交通里面啊,有一些手语的识别,比如在机场的时候,告诉一个,机场说这个飞机倒进库的时候,他通过是手语来表示的,这个技术呢,这个技术就可以帮你识别这些,指导员的手语的是讲什么样子,聋哑人也会使用手语,但是呢手语如果你没有学过的话,你也不知道是怎么回事,那么这个技术呢,就帮助你跟使用手语的人进行交流了,在这个应用里面,他有这里举了两个数据集,第一个数据叫做 pck,他是一个在Flickr上采集的2,000张图片,他是关于一些运动员的一些姿势啊,然后让你去判断14个不同的关节的一些位置,可以看到现在这个数据集上在,过去一年有一个非常大的进展啊,虽然又平了一段,但突然一下就跳到了99.5%,基本上这个数据就解决完了,下一个数据集讲的是一个3D的人的一个姿势的识别,他是一个人啊,然后拍了很多张不同角度的照片,然后里面有17种啊,不一样的一个就是比如说你在打电话,还是你在,抽烟啊或者在聊天,然后让你去判断那些关节点的位置啊,和真实他的位置的一个误差,在2014年的时候这个误差还是比较大的,这个是 y 轴是厘米,而到了去年的话,这个误差基本上到了2厘米左右,就是说我判断出来你在空间中的位置,和你真实的位置的差距是2厘米,就那么一点点,这基本上是非常准确了,下一个任务是语义分割,这个任务里面呢,你要干的事情是说,对图片的每一个像素,去判断他是属于哪一类,比如说,这个地方是一个骑摩托车的视频,然后你要判断说啊,黑色的那些像素都标记成了背景,然后红色的是摩托车,绿色的是人,这个应用呢主要用在比如说无人车了,你要去判断说,你看到的地方,什么地方是你可以开的路面,什么地方是行人道,什么地方是建筑什么是天空,或者说做一些图片的分析啊,比如说你看图片哪个是前景,哪个是后景,现在大家在做视频会议的时候,通常会有一个选项叫模糊你的背景,他就是用的是语义分割这个技术,如果你用手机拍照的时候,很有可能你也用过一个叫做背景模糊的一个效果,就是模拟出相机的那个景深的效果,他很有可能也用的是语义分割,然后他还举了一些别的例子啊,比如说在医疗诊断里面,拍的照片里面判断有没有肿瘤,如果有的话把它割出来看看有多大,对这个应用他用的数据集是CityScapes,也是在语义分割里面最常见的一个数据集啊,他是在50个城市里面,在里面开车然后录下的一些视频,然后做了一个分割,然后他们用到的是一个叫做 iou,的评测标准,如果你上过我们的课,也知道他大概什么意思,但是直观上来说比较难解释,就是说你的具体某一个 iou,和到你实际效果的一个区别啊,可以看到是说进展是比较迅速的,然后在过去几年,还是我们在持续的提升这个效果,特别是绿线啊,表示的是如果你用了额外数据集的话,在上面还是有提升的,当然你可能很难说86%,到底代表是什么意思,但是你要反过来,来讲说这个技术在无人车里面有用到,现在无人车呢也算安全吧,所以这个技术也是差不多够用的,当然无人车还用了很多很多别的技术啊,他有更好的传感器来做更好的分割了,在语义分割之下呢,然后他又讲了一个下面的一个子任务,就是对医疗图片的语义分割,具体来说我给你一张医疗的一个图片,然后你要判断每个像素到底是属于哪一个器官,然后他讲了两个数据集啊,就这两个数据集可以看到,精度也是在一直的提升,现在是94%一个是92%,但是这个地方,我们只能说在过去几年里面,我们是有提升的,但是离实际的使用到底有多远,这个我也很难看清楚,因为因为你没有说到底 AI 的专家,他的在什么地方,可能在这个地方或可能是更高了,而且医疗这一块是非常敏感的,就是说误诊可能问题还没那么大的,比如说没有病的人你认为他有病,然后然后去做复查,那么也就是可能是浪费一点钱,这个不会造成特别严重的影响,但是比较严重的是漏诊,就是说这个人可能有病,但是你没有检查出来,然后然后结果之后发现是有问题,但是可能就错过了最佳的治疗时期,所以在用 AI 做决策时候,特别是跟人很相关的时候,要特别特别的小心,接下来讲呢是人脸的检测和识别,检测是说在图片里面把人脸找出来,识别是说看你到底是谁啊,这个技术大家应该很熟悉了,在用手机的时候很有可能有啊,人脸的解锁,然后你可以用刷脸来支付,然后检测这块,我发现新相机的这个人眼对焦,非常的厉害,对于高速运动中的人啊,即使你在画面总是很小,你也可以很准确的把他眼睛找出来,进行对焦,这个是计算机视觉里面做的最早,也是做最多的应用之一了,然后可以看到在过去几年里面,对人脸识别的一个检测率啊,的误差还是在不断下降,而且这是一个 log 级别的,但是因为人脸技术啊,被大量的实际应用中使用,也带来很多安全性的问题,特别是隐私的问题啊,最近有很多国家出台法律说,禁止人脸技术啊在公共场合被使用,因为疫情的缘故,人脸检测呢又出了一个新的变种,就是戴了口罩的时候,人脸的检测和人脸的一个识别啊,这有个数据集啊可以看到是说,如果你戴了口罩的话,那么你的错误率,其实比你没有戴口罩还是要多很多的,虽然在过去两年里面,我们做了很大的进展,但是目前仍然戴了口罩的啊,误差率比没有戴口罩误差率还是要多7倍的样子,苹果说他最近一次的手机更新里面,可以允许你戴了口罩进行解锁,但我实际上从来没有成功过一次吧,下面一个任务是叫做视觉的一个推理,这里面有一个有名的任务,叫做视觉的问答,就是说我给你一个图片,然后问你一个问题,然后你要判断你的答案,比如说这个下面这个图片,谁带了一个眼镜,这个是一个男人带了啊,这个地方是女人带了,然后然后问你,下面这个图片的雨伞是不是向下的,这张图片是这张图片不是,在这个叫做 vqa 挑战的数据集上,可以看到,过去几年我们的进展还是比较显著的,这个是人回（复）的一个精度,可以看到在去年我们基本上可以做跟人回的精度很像了,但也就是在这个数据集上做的很像,但是在真实的里面,如果你要回答比较,难的问题还是比较难的,因为这个数据集还是只是一些很简单的问题在里面,虽然看上去在这个数据集上,我们的模型和人的精度差不多,但实际应用中,我们还是有很长的距离要走的,接下来是视觉里面的视频的部分,在视频里面一个重要的任务叫做行为识别,对应在你图片里面的物体识别,就是说给你一段视频,然后你判断里面在干什么事情,比如说你里面是不是有个人在走路啊,有个人在挥手啊,或者站在这里,或者跟别人在说话这样的场景,这个这个里面有名的数据叫做 kinetic这个数据集,它是在 YouTube上找了几十万个视频,然后呢把它分类到不同的种类里面,然后四百六百,700的意思是说这个视频一共有400类,600类和700类,所以当然700类会更难一些,下面是一些样本啊,啊比如说这个视频呢,这些那些帧是表示在握手啊,这个表示的是在拉伸啊,这个表示的是在。。。,然后这个图呢,表示的是在四百,六百和700的三个数据集上,按照年份他的 top 1的精度他的变化,可看到400和600的精度很像,基本上在去年得到了89%的精度,然后在700这个数据上呢差一点,但是也不差了82%了,下面一个数据集呢,是一个稍微难一点的数据集,他说我给你700个小时的视频,然后呢里面有 200个不同的行为,但是你需要给我找出,从哪是要开始到哪是要结束,是一个行为,而且那是什么,就是说你不仅要去判断出是什么行为,而且你给我找出来在视频中的位置,大家可以看到虽然我们是有进步的啊,但是在过去几年好像进步也,还不是那么的大,而且最终的进度我们也只达到了44%,所以这里面还是有很多的空间可以做的,接下来是物体检测,我对把物体检测放在,视频下面觉得很奇怪,因为现在讲的是,计算机视觉的视频部分,我觉得这个应该是放在图片里面的,让他给的样例是 coco 的数据集,这个数据集是没给错的,但是我觉得这些例子,他是语义分割或者实例分割的一些样类,coco 里面当然是有这个任务了,但对于物体检测来讲,我们一般是画一个物体边界框,所以我觉得可能是作者啊小编啊,在放的时候放错,而且复制的时候复制错了,首先这一章啊他的技术含量并不高啊,其实他就是把这几个数据集拿出来解释一下,然后所有这些图呢,你虽然你看上去很好,其实他就是来自于啊,paper with code 就是你看他的 Source是这来的,就说你把 paper with Code 里面的那些数据扒下来,然后把它重新画一下,所以技术含量并不高,而且他是做了简化呀,你去看 paper with code,你能看到每一个点是在干什么事情,而且具体是说,你每个点对应的是哪一个方法,他是在谁做的,你能仔细的看到那些文章是谁,这个地方你把那些方法全部挪掉了,这也是因为这篇文章啊针对了是一般的读者,大家对这一特定的技术 特定的模型,不那么了解他们只想知道一些趋势,到底 AI 在这个应用上做了怎么样,是不是有进步,离我们最终的目标多远,就是给一个大概这样子的啊,印象就行了,然后看到在目标检测这个数据集啊,用的是 coco 这个数据集,这个数据集呢是,您可以认为是在目标检测界的地位,相当于 ImageNet,在图片分类这个任务中的,因为这数据集1呢做的比较早,第二呢他确实量还是比较大的,可以看到在过去几年,我们在 coco 这数据上,提升是比较稳定的,特别是你如果,能够使额外的数据的时候,我们的一直在线性的往上增加,目标检测在计算机视觉里面,应该也是最热的方向之一吧,可能也是在计算机视觉,用的最广泛的应用之一,所以有大量的研究者,投入在这个方向,而且进展还不错,达到了80%左右的一个 map,但是这个地方已经很高了,因为你算的是你预测,出来的那一个物体的边框,和你真实标的那个边框的一些距离,啊但是这个地方呢你差一点点,其实问题不那么大,但是根据趋势来讲,我们看到应该,在未来几年还是会持续的往上走的吧,在计算机视觉部分呢,最后一个样例,他给的是一个视觉的常识的一个推理,具体来看可以看一下这个例子,这里面有几个人啊,其中这个人呢,拿一个手指指着另外一个人,他就问你说,为什么,第四个人在指着第一个人,这个难度还不是那么高,因为他给了你很多选项,你在选出正确的事,不是说让你去回答他到底是怎么回事,可以看到这是人类的一个基线,这是你的模型的基线,在过去几年,这个方向相对来说比较小众一点,所以好像进展没那么迅速,然后跟人类还是有一段差距的,好了这个就是计算机视觉部分的介绍,接下来我们看一下自然语言处理,在语言的部分呢,他首先讲的是英语语言的一个理解,他具体用到的是一个叫做,superGLUEGLUE 的一个数据集,这里面呢其实有很多个子数据集,我们可以看一下不同的任务他干的事情,第一个呢是说我给你一段文字,然后问你一个问题,你回答是还是不是,第二个呢是说,我给你段文字,然后我提出一个猜想问你说,然后让你回答说,这些文字能不能支持你的猜想,第三个跟前面的类似,就是我提出一个前置,这里说的是我的身体在草地上有一个,阴影然后他有两个,然后他有两个选项,第一个是说太阳在升起来了,然后第二个是说这个草被割掉了,那么正确答案应该是第一了啊,这个数据集呢,包含了很多个这样子的子数据集,然后最后算平均分数,会看到过去几年里面,特别是很大的这些Transformer的模型出现,使得模型能够超过一个人类的水平了,那这里面另外一个有名的数据集是,斯坦福的 q a 这个数据集,然后这个数据集呢有两个版本啊,不管是 v 1还是 v 2都会看到,其实还是超越了人类的现在的水平了,在文本理解以外呢,另外一个英文叫做文本的摘要,然后给你长长的一段话,让你把中间的重点给摘出来,这个在大家看新闻的时候可能看过,就是这个报道的一个摘要,就要看了一段话就行了,而且我们在读论文的时候,这个也是很常见对不对,我们有个摘要的这个环节,就是说如果你不看论文,你就看一下我的摘要,就能够大概知道是怎么回事,所以在这个地方,他一个著名的数据集,就是 arXiv这个数据集了,他把所有的论文爬下来,让你根据你的正文来预测你的摘要,他这个地方用的评估呢,是一个叫做 ROUGE的一个东西,我们没有仔细讲啊,基本上讲的是说,就是你生成的摘要和你的,数据集提供的真实的摘要,里面的一些子片段的一个重合的一个评估,可以看到我们还是有一些进展的啊,但是在过去两年好像进展是一般,而且你也不是很好判断,到底48%是一个什么样的意思,到底他摘下去的东西能不能读,所以我觉得这一块,我们还是有很多空间可以去做的,另外一个任务叫做自然语言的推理,啊比如说我给你一句话,再给你一句话,你判断下面一句话和上面一句话到底是一个冲突的关系呢,还是说他没有什么关系,还是后面一句话是前面一句话的一个增强的关系,所以在这个三分类的任务上面我们现在的进度是93%,还是挺不错的,另外一个是一个稍微难一点的推理任务,叫做拓展推理,可以看到这个例子啊,就是说他会有两个观察,一个是说今天的天气特别好,第二个是说,他要他的邻居做一个 jump stars,就 jump star 的,是说你用你的车的一个电池,接到我的车的电池,把我的车发动起来,所以这个地方他给了两个选择,一个是说 说Mary想开车去海边,但是呢他的车呢不能开起来,因为他的电池坏掉了,第二个时候他开始的时候,有一个很奇怪的声音,那么基本上可以从前面两句话我们能够猜到,应该结论是这一个啊,所以这个任务比之前那个人要难一点,是说这里面有想象空间更多一点,这也更符合人类的一些日常的交流,就是说我跟你说一句话,那么我马上是可以,拓展到一些别的点上面去了,在这个地方呢,在这个数据集上,我们看到 我们的模型也取得非常好的进展,基本上跟人类的精度也快差不多了,当然这还是一个很简化的问题,就是说你在几个选项中间选一个对的,而不是说让你真的把这个选项给你找出来,下一个是另外一个分类任务,就是情绪的一个识别啊,通常我们要判断说,一个商品的评论,是正面评论还是负面评论,我们大概就知道,整个大家,对一个商品的一个好坏的一个判断,或者说大家对未来的一些,是正面还是负面,也是能帮助,而且这些汇总的信号,对于财经市场通常是一个重要的信号,所以这个应用呢也在被大量的使用,可以看到这个精度啊,在过去的几年里面发展也还是不错的,现在我们能达到一个88%的精度,也是基本上是一个可用的接受范围啊,然后在机器翻译里面呢,在这个应用里面最有名的数据集叫 wmt,这是14年在 acl 引入的一个数据集,然后这里面有大量的一个,语言翻另外一个语言的,这些文本,这里面最大的两块,一个是英语翻德语和英语翻法语,但翻译是交互的你反过来也是 ok 的,他这里面评估了,一般用的是一个BLEUBLEU的分数,他是判断出来你翻译出的句子,和你真实的句子那些子序列他重合的一个 个数,我们之前在上课也讲过,他具体是怎么计算的啊,可以看到是说在过去几年里面,我们的分数也是在有提升啊,特别是这个是有是额外的训练数据会怎么样,当然呢大家可能也不是很理解,44这个分数到底对应的是一个什么样的水平啊,大家如果使用过这些,机器翻译的一些服务的话,大家也能理解到,如果是一个比较常见的句子或正常的句子,翻译还是比较靠谱的,但如果比较冷门一点,比如说我要翻篇论文呢,或者一些专业领域的翻译的话,稍微差一点,在 b 站上有很多这样子的视频啊,比如说我把一些古装戏,比如说诸葛亮的出师表拿,然后来回翻译之后,基本上的语义就全部变掉了,所以机器翻译目前来说是,在正常的情况下是能用的,但是在一些边角情况下,当然还是有各种问题了,在过去一些年里面一些商业的服务啊,或者是开源的服务的一个区别,那有一些是跟之前的不一样,这一块并没有太多的开源的这种预训练模型,而是大量的是商业的一些模型,因为毕竟翻译这个事情是有很好的商业模式啊,比如说十几年前二十年前,你去一个打印店的话,你可以看到说翻译多少字多少钱,所以这一块,如果你真的能做到跟人的,翻译水平差不多,而且更加便宜的话,那么当然是有很大的商业前景,所以为什么是商业系统是里面的主流,但反过来看,现在也就是38个商业系统啊,这是因为,这个世界上,的主料语种可能就那么几十种,如果你能做出个系统,把这些主流语种之间相互翻译,已经做的很好的话,那么别人就活不下去了对吧,因为不像之前的视觉,比如说物体检测,你有那么那么多的物体,你很难有一家人把所有的物体给你检测做很好,所以在不同的垂直领域,你可以允许不同的商业系统的存在,但是在机器翻译的话,一般还是大厂的游戏吧,2.4讲的是语音,语音里面最重的一个任务,是语音的识别,就是说我讲一段话,然后你能把我这些讲的话,识别出来是讲的哪些词,比如说这个数据集提供了1,000个小时,读书的一个数据集,就是有声读物的数据集然后呢,你根据你那个音频来识别出来,那些文字到底是什么,他提供了两个版本啊,一个叫做干净的版本,就是说你的信号是比较干净的,第二个是说你的信号是不那么干净的,比如说你很多背景的噪音,那在干净的上面可以看到,在过去几年啊,进展还是不错的 一直在往下降,而且降到去年的话,已经是错误率是1点 4,就是说100个词里面,我的错误的概率是1.4%,那么离真实使用已经很近了,那如果你有噪音的话,也就是2.几%的一个性能呢,那看上去也是能用的,其实语音识别是深度学习,有所突破的第一个例子,就是说最早的深度神经网络,是在语音识别上取得了突破性的进展,在过去的十年里面,我们看到语音系统啊,他的准确率啊一直在往上升,就是错误率在一直在往下降,而且大家可能对他也不陌生了,比如说你跟语音助手对话的时候,他就要做语音识别,有时候你在微信里面,你给别人发一个语音信号的话,别人可以通过一个语音转文本的,功能把你这个段话的文字给显现出来,也会多少用到一个语音转录的一个功能,比如说我们做视频生成字幕的时候,也是多少基于语音识别的,但是大家用的比较多的话,还是可以发现很多问题吧,比如说你的背景真的很多噪音的时候,识别率还是不那么高的,而且如果里面有很多专业词汇的,比如说我们这种视频啊,他基本上是识别不出我们讲的那些专业的词汇的话,而且你得把它矫正过来,或者说你有口音的话,那么你识别率也会下降,比如说我的,比如说我讲中文的识别率,是远远的高于我讲英文的识别率的,2.5讲的是推荐系统,他讲的两个数据集第一个是 MovieLens,MovieLens 是一个电影推荐的网站啊,然后他爬了2,000万个,哪个用户看到哪一个电影的,这样子一个记录,然后让你来推荐,说我要把哪个电影推荐给哪个用户,然后可以看到,从2018年到2019年还是有提升,但是过去几年好像进展不那么迅速,但我觉得这个数据集呢,就不是很能代表推荐系统这一面东西,但是我觉得推荐系统啊,跟你的业务实在是太相关了,一般也就是大厂干的事情吗,你没有几百万几千万个用户,你做推荐也意义不大,所以这个时候,如果你有那么多用户的话,那么真的是,根据你的公司的产品的形态,你是推荐短视频呢,还是推荐新闻呢,推荐商品呢,还是推荐别的东西,那么你这些用户的行为不一样,你的商品行为不一样,那么那么导致你的算法的选择也是不一样的,所以我觉得这个线看上去很平,但是不能代表说在整个推荐系统,在各个公司的应用上,他的性能是这样子的啊,同样的道理也对应下面一个叫做广告点击的预测,他用的是一个 Criteo的数据集,然后他呢去点击说,一个广告用户会不会去点,然后可以看到他的 auc 啊,是在往上升的,但是还是一个道理,广告点击呢真的也是大厂的游戏,世界上的广告公司也就那么多啊,但是根据你的广告的不一样,你的用户不一样,然后你在展现的地方都不一样,所以导致里面有,所以导致里面大量的信息是可以挖掘的,虽然你看上去Criteo,这是一个相对来说比较简单的数据集,他的进展是一般,但实际上在真实的广告系统的里面呢,里面要考虑的东西是方方面面的,所以我觉得这种公开数据集啊,他不能很好的反映出,在真实的应用上的数据集的一个情况,第2.6节讲的是强化学习,强化学习跟我们之前的方法,不一样的地方,在于他需要有一个环境,这个跟人的学习有点像,比如说人学习走路的时候呢,你每次想着说,我要迈左脚还是迈右脚,然后你做了一个动作之后,你会得到一个环境给你的反馈,你有可能是往下摔,或者是往前真的走一步,然后你根据你的反馈,再去调整你下一步,所以强化学习呢,也就是每一次你的模型做一个行动,然后你的环境告诉你说,你的行动是有奖励还是有惩罚,所以这个地方,环境可以认为就是你的一个数据集了,比如说他这里举了几个例子,第一个是Atari57,就是在Atari这个游戏平台上面的57个游戏,包括了大家熟悉的吃豆人呐这种游戏,然后你的模型呢,就控制你的游戏机去玩游戏,比如说在吃豆人里面,你吃了一个豆子就得到一个奖励,那么你吃的一个,水果然后再把一个怪物吃掉,那么你就会得到一个更大的奖励,如果你被怪物吃掉了,那你就得到一个惩罚,然后你根据这个环境给你的反馈啊,不断的去调整你的算法,然后你的最后的目标就是能拿到更高的游戏的分数,这个环境在2013年出来之后啊,我们可以看到最近几年他的分数,在2017年开始有一个突飞猛进的一个发展,这应该是 deep mind 吧,这一块带火了,导致很多的研究者进来玩游戏,另外强化学习的一个经典例子就是下棋了,这里举的是下国际象棋的例子,可以看到在80年代我们就开始了,而且系统的性能是在不断提升,然后在2006年的时候,应该是超越了人类,最好的国际象棋的选手,现在是还是在不断的往上提升了,如果现在来看,大家可能更关系的是下围棋了,因为围棋他的搜索空间要比国际象棋要大很多,但他这里没有举围棋的例子啊,主要是因为,这可能是,因为围棋没有一个标准的方法来衡量,各个算法之间的一个差别,所以无法画出一个这样漂亮的图,但我们要知道早在几年前,围棋应该已经打败的人类,而且现在在围棋界,大量的使用人工智能来帮助训练棋手,柯洁也在一些天前感慨说,我特别的恨 ai,这也代表了时代的一个变化吧,2.7讲的是一些硬件,他首先第一个讲的是 ml perf,ml purf 是一个工业界的评测级来评测,来评测各个不同的硬件系统和软件系统,在不同的模型上,达到特定的精度的时候所花的时间,这个地方可以看到是说,啊在不同的年份,你的一个任务的训练时间的下降,注意到这里 y 轴是你的一个 log 尺寸啊,他不是一个线性的,所以如果这一条是线性的话,那么就是指数级的往下降,但这个地方稍微比指数差一点点,但是已经也挺不错了,这个地方可以看到是你的目标检测,再下面看到的是你的图片识别,所以可以看到在 ImageNet的上面,我们基本上可以在23秒的时候,把一个模型训练出来,就算是在更复杂一点的目标检测数据集上,我们现在也只要几分钟,就能把一个模型训练出来,就是已经非常非常快了,当然在日常生活中,我们可能会,训练比 ImageNet 更大的数据集,而且呢我们也不会像 ml perf一样,用那么多台机器来训练,所以你所以正常情况下,还有可能是分钟,或者是小时级别的时间吧,但仍然是在非常可以承受的范围里面了,这个图呢画的是一个任务啊,最多用了多少个加速器,一般来说加速器就是 gpu 了或者 tpu 了,然后可以看到是在2020年的时候啊,大家最大的是用到4,000多块卡,有意思的是说,你并没有看到这一个还在往上增加,应该是有两个原因吧,第一个原因是说你的数据集就那么大,如果你要用更多的机器的话,那你的系统的优化就更难了,毕竟你的任务就那么大,然后你用更多的机器,每个机器分到的任务就会变少,这样子的话,你的分布式系统的优化更加难一点,第二个是成本确实是挺高的,你要想我如果用4,000块卡,这个地方一般用的是 gpu 的服务器卡,你去买的话,就是1万美金或者7万人民币吧,那么你用4,000块卡,你就光是你的 gpu 的话那就4,000万美金,如果你再配上其他的话,那可能加起来就是1个亿美金了,就是说你用一个,一个亿美金或者7个亿人民币的系统,去测一个算法性能的话,也真的是要有钱人才玩得起了,下面这个图呢,讲的是我要在 ImageNet上训练,得到一个93%的 top five 精度的,成本是什么样子,从2017年的1,000美金开始,下降到现在的不到5块钱呢,这里面当然是有很多因素在帮忙,一个是我们更快的系统,更快的优化算法,另外一个是硬件成本,确实在不断的下降,当然呢,我还是有点怀疑这5块钱是怎么算出来的,我觉得可能是在极度理想的情况下算出来的吧,实际上你可能还是比这个高一点,但是不管怎么样,5块钱还是非常的可以承受了,好,第二章的最后一个章节讲的是机器人,具体来讲,他其实讲的是机器人手臂的一个价格,我们知道,机器人其实包含很多很多东西,但是最常见的是说你弄一个机械手臂,他用来抓东西或者帮你做一些事情,而且在工业界的话确实,所谓的机器人生产,也就是一个机械臂在帮你生产,然后这个地方他画的的是说,不同的年份平均的机械臂的下降价格,可以看到是基本上在过去的五年里面,降了一倍吧,这个来自量产的缘故和工艺的改进,然后这个地方画的是你每一年,不同的机械臂的型号之间的一些点,每个点代表的是一个机械臂的价格,然后呢中间这个线是你的中指数,可以看到特别便宜的机械臂,可能就是几千或者几百美金呢,可能每个人都是能够买得起的,最后作者问了一下,做机器人的这些教授啊,他们一般用什么样的技能啊,回答的主流的是深度学习,然后第二是强化学习,当强化学习和深度学习,结合就是深度强化学习,这也是强化学习的一个主流啊,好第二章我们就介绍完了,可以看到是说在各个不同的应用上面,我们的模型都在不断的提升自己的性能,而且在一些应用上,已经超过了人类的水平啊,同时我们的训练的成本,硬件的成本也在往下降,但是同时我们也说到啊,这些任务是相对来说比较简单的任务,跟具体人能干的事情还是差很远,这些任务大多只能对应于人的感知,比如说识别一个图片里面的东西,或视频里面的东西,或者做一些很简单的自然语言的理解,是人类在几秒钟能够干的事情,但是对于人要花几个小时,甚至很多天的那些任务呢,我们暂时是没有涉及到的,从内容上面啊,我们也讲到这里面的大部分的数据,来自于 paper with code,里面还是有一点小瑕疵啊,比如说我觉得,比如说之前的目标检测,就不应该放在视频那一部分,而且在推荐系统里面,我觉得那两个数据集,是不能代表真正的行业上的进展,而且也没有特别介绍,这些新的结果到底是由哪些工作完成的,如果大家有兴趣的话,可以去 paper with code 的这个网页上看一下,每一个大的领域下面其实他有,上千上百个不同的子任务,你可以点进去看到,对每个数据集,到底每一年是哪个方法是最好的,你可以看到所有的这些,具体的值和用的是哪个方法,第三章呢讲的是 AI 的伦理,那伦理这个词啊,就是比较的模棱两可了,他在他在导言里面,讲了一下伦理关心的一些事情,他最主要的关心的是一个模型,可能会对人类造成的伤害,比如说一个商业的人脸识别系统啊,可能有种族的歧视,所谓的种族歧视就是说,你可能对白人的,男性的人脸的识别率会高很多,但是对于比如说黑人的,女性的识别率就会低一些,或者说你的简历的筛选系统啊,是歧视年龄,比如说如果你年龄比较大的话,我就认为你不适合这个职位,或者是一个用 AI 驱动的健康工具啊,可能在背后会区分你的经济地位,当然你也不能否认说一些职位,可能某一个特定的年龄段的人更适合,以及你的经济情况,跟你的健康可能确实相关的,但是如果你的系统啊,过分的依赖于这些因素,而且这个系统,确实在部署在实际生活中的时候,那他可能会加速社会的分裂,所以 AI 的伦理呢,就会关注一些公平性和偏见,以及有什么样的办法去,提升你的公平性,和降低你模型的偏见,他在下一段也大概解释了,公平是一个什么意思,他说一个系统如果是公平的话,就意味着是说你把一些跟你的种族啊,年龄啊性别相关的因素改了之后,不会影响到你的结果,比如说我把一个简历丢给一个系统,去判断我是不是适合这个职位,如果结论是 是的话,那么我把我的简历里面的性别改成女啊,或者把我的种族从亚洲改成黑人的话,应该也不会影响到你的结果,当然啊这个领域是比较新,也不是所有人的认可这样子的定义,我们在这里不做特别大的展开,下面这个图讲的是说在过去一些年啊,关于公平和偏差的一些指标,就是说我要,去衡量一个模型的公平性和,有没有偏差的时候,我要去提出各样的指标,我们在之前看到各种啊精度呀,auc 呀或者别的,但是呢因为公平和偏见这个东西,不是那么的好定义的,所以在过去些年,大家提出了各种各样,不一样的指标来衡量模型啊,这个是你每一年提出的指标个数,就加起来,你可以看到可能就四五十个都有了吧,下面讲的是用来测试公平 偏见的一些数据集,和一些诊断的指标,也看到在过去几年,特别是2019年,我们有很多新的这样的东西出来,接下来我们来看一下,在 NLP 里面的一些具体的偏见指标,第一个指标叫做毒性,所谓的毒性就是说你一个文本,他是不文明的,不礼貌的啊,然后这个图呢,讲的是在不同的数据集上面训练出来的语言模型,人家这个语言模型呢,生成文本的时候呢,他生成出了有毒的那种文本的概率是什么样子,可以看到不同的训练样本,他造成的毒性是不一样的,这个是用书本的数据集,这个是一个网络上爬的一个文本集,但是呢他把里面有毒的那些文字,已经过滤掉了,所以导致整个语言模型出来是比较干净的,这个是没有过滤的情况,你可以看到这有个巨大的一个增幅,这个是 g p t 3用的数据集呢,这个是Wikipedia 的数据集,所以可以看到对语言模型来讲,你的文本的好坏当然是非常重要的,接下来是一个 DeepMind的工作,是说不同大小的语言模型,对于毒性的一些敏感度,这下面是一个比较小的模型,则是一个亿的参数的,到后面是比较大的是到了2,000亿了,然后不同的线呢表示的是说,我给语言模型啊前面那一个引导符,那里面如果有些引导符,是比较容易出来一些,有毒性的文字,哎比如说中国足球了,对于这种容易引起高毒性的这些,前置的文本啊,随着你的模型的增大,然后出现真的有毒的结果的概率,是有显著的增加的,也就是,也就是大的模型更容易受到危害一些,当然现在有一些方法,来给语言模型消消毒啊,但是呢一旦用了这些方法的话,会导致你语言模型的效率会往下降,这个地方表示的是,用了三种不同的消毒的方法,然后跟你的基线比,就是这个红色的笔,在不同的文本上面啊啊,他的模型的复杂度,得这个地方是越低越好,可以看到不管是哪一个数据集上,只要你用上了后面的任何一种方法,你的 proplexity 都有显著的增加,也就是你的模型性能下降了,另外一个重要的指标叫做刻板的印象,就是说你的模型对你的性别啊种族啊,宗教啊他们有特定的一些偏见,下面这个图呢表示的是说,当你的模型变得越大的时候,就同样一个模型,当你的学习的参数变大的时候,你的刻板印象是在往上增加的,你看他比小的那个模型是有增加,在这个地方,然后后面这个图,对比的是bert和后面的两个改进方法,在不同这些属性上的区别,包括了年龄的残疾,你的性别,国籍外貌,种族和肤色,宗教性取向和你的社会经济地位,可以看到这些分数都还挺高的,就是在60-80之间呢,另外一个实验呢 调查的是说,我假设把一句话里面啊,跟你性别相关的代名词去掉,比如说 he 或者she 我把你去掉之后,让你的语言模型把它填回来,是不是你能够填到正确的 he 还是she,然后这个地方,表示的也还是不同大小的语言模型他们填空的那个精度,模型大一点的话,好像还原度会更高一点,这个地方有72% 70%的一个精度,但是小的模型呢,确实对于性别,那一块的敏感度没那么高,所以这也是一个双刃剑啊,就是你的更大的模型对性别的那一些,学习的更好一点,但一呢容易引起这些偏见,第二呢他确实能够更好的去,把你这个性别准确的性别填回来,然后这是一个类似的在机器翻译里面,你把一个含有不同性别代名词的句子,从比如说英语翻到一个别的语种,再翻回来看一看那个性别有没有搞错,3.4呢讲的是如何用 AI 来判定虚假消息,最近的俄罗斯和乌克兰的战争,也大家看到了大量的虚假消息啊,所以这里讲的是说如何用 AI 来判断,网上讲的各种东西,是不是真还是假,所以这里是说不同判断的种类的那些数据集的增长,可以看到从2017年开始,在过去的几年里面,在这一块是,数据集上是有个显著的增长的,下面是一个单独的一个案例分析,讲的是 clip 这个模型里面的一些偏见,在讲 clip 模型的时候呢,我们其实也提到过,他作者在之后其实讲了挺多的,这些局限性的,这个地方他要做了一定的展开,比如说研究者又做了一些实验,他让 clip 去对一个 Fairface,就是人脸的一个数据集做一个分类,但是在标号里面呢,增加了各种名词啊比如说啊犯罪的啦,啊嫌疑人啊,或者是猩猩这样子的名词进去,然后他发现说,对于黑人的脸来说,你CLIP认为,这些标签的概率更大,的情况出现的更多一些,也就是也就是说,CLIP更容易把黑人的脸和这些非人类的这些动物的名字和,和犯罪相关的名词呢,关联的更紧一些啊,另外因为 CLIP,可以去判断任何一个文本和一个图片之类的关系啊,研究者发现说如果你是 housekeeper,就是做家务的人的话,那么更多的是跟女性相关一些,如果是你是,如果是个监狱犯的话,那么跟你的男性相关的更多一些,最后他给了一个例子,让CLIP来判断一个文本和一个图片之间的相似度,这个图片呢当然就是一个女宇航员吧,如果说这是一个宇航员,的一个照片的话,但相似度 还没有这句话,就是说这是一个笑着的家庭主妇,然后穿着一个橘色的衣服 来的高,好这就是这一章,我们看到的是各个不同的模型,在各种我们关心的,关于公平性和偏见性的指标上的一些表现,当然这也不公平,说某一个模型是不公平的更多是说,更多是说,你用来训练这个模型的数据集,可能是带有偏见的,这个数据集呢,来自于我们采集到的数据,所以或多或少的是说,这个社会本来就是,不公平的,也就是说不同的年龄,国籍性别,肤色的人,不是随即的分配在各个职业上面,而是每个职业都有自己的一个,聚类的效应啊,比如说 Wikipedia上面的贡献者,绝大部分人是男性也,所以很有可能把,下意识的这些性别的偏见,带到 Wikipedia里面,然后在 Wikipedia上面训练的语言模型,当然也会带有了这样子的性别的偏见,所以这一块我们还在研究的早期啊,也能看到最近很多文章,在训练好模型之后,也会去调查自己模型的公平性,应该是一个比较好的开始吧,但是后面我们还有很长的路要走,第四章呢是关于经济和教育,如果你是想找工作或者是想,选专业的话,这一章可能跟你更相关一些,首先这个图呢,表示的是在不同的地区里面,然后呢所有在 linkedin 上的招聘帖里面,要求有 AI 相关技能的百分比,有意思的是你看到新西兰是最高的,有2.4%的啊,招聘贴里面要求就 AI 的技能,国内和美国呢分别在这个地方,但我总觉得这一块呢,就不是那么的准确,首先在国内可能大家不用Linkedin,第二呢在美国呀,大厂啊都有自己单独的网站来放招聘的信息,所以他们也不会,把这些放在 linkedin上,所以这个所以这个呢,会导致在计算上都有偏差,而且整个章节关于就业这一块啊,都是用的 linkedin的数据,所以呢也提前打个招呼就说这一块呢,不代表特别的准确,只是说大家看一看就行,下面这一个讲的是 AI 相关的职位里面,都在哪一些国家和地区,有意思的是,新加坡占了大头啊,这个有点不理解,新加坡就几百万人口啊,但是他 AI 的职位他占了,整个世界的2%,ai 相关职位里面对技能的要求,可以看到大头是机器学习,但人工智能,我觉得这相对来说不那么专业一点,因为人工智能这个里面太广了,你到底什么算人工智能对吧,第三个是神经网络,接下来是自然语言处理,有意思的是,自然语言处理比视觉还能多那么一点点,毕竟可能公司跟文本打交道的,情况更多一点,有意思的是无人驾驶也能够上榜啊,然后是哪些行业在招 AI 相关人员,是信息技术行业以及专业服务行业,制造业金融业,农业啊等等啊,这里画的是美国的各个州啊,关于 AI 职位的一些个数,可以看到最大的是加州,其次是德州,下面是一个叫做 AI 的渗透,就是说对于 linkedin的用户,然后去统计他有什么样技能,然后看他有百分之多少是 AI 相关的技能,可以看到是排名第一的是印度啊,印度人民是把最多的 AI 的技能,写在自己的 linkedin的介绍里面,接下来4.2讲的是一些投资,可以看到在过去一年里面,主要的投资来自红色这一块,就是收购与并购,以及蓝色这一块也就是私有的投资,比如说公司的 a 轮 b 轮 c 轮,而且确实在过去几年里面,增长也是很快的啊,这个是私有投资在每一年的变换,其实我在2015年也是博士毕业的时候,去创过业也拿过投资,当时候觉得那时候已经很火了,非常多的人想送你钱,但没想到在过去的六七年里面,这个投资又翻了将近十倍,特别是去年一年整个就是翻了一倍吧,在这一块上面,虽然在过去几年面,大家都说我们要过冬了,要存储现金,但实际上呢在去年投资上,大家是一点都不手软,然后这个是在每一年,新成立的 AI 的公司,可以看到的峰值在2018年,但是从它之后呢其实逐年下降了,在投资排名的国家和地区上面,第一是美国,第二是中国,中国比美国呢大概是1/3的样子,中国大概是美国的1/3的样子,所以呢如果你要创业的话,大概是在中国,或者美国是不错的选择了,然后看到这些创业公司都是,在做哪一块,在过去一年可以看到,排名第一的,是数据的管理和云上的相关,因为疫情的缘故啊,大家大量的数据都要往云上走,这样子我的,本地的 it 的开销会更低一点,而且呢在管理上啊,通过远程啊也更加方便一点,所以在过去一两年,确实有很多这样子的公司出来,让排名第二的呢是医疗和健康,接下来是金融,然后是自动驾驶,在历史上累计来看,排名第一的是制药和健康,其实数据的管理和处理云,工业自动化和自动驾驶,所以相比之下,你可以看到我们最近的趋势是真的在,啊数据的管理和自动驾驶的上面,而且你可以从下面这个图的趋势,可以看到啊,在自动驾驶,在数据的管理和云啊,金融,制药和健康上面这几个行业里面,在 AI 的,这些 AI 的创业公司增长是最快的,下面这个图呢讲的是在工业界啊,所有人对这些,ai 带来的风险的一些担心,首先排名第一的是你的安全啊,就是不要被黑了,第二呢是要符合法规,是因为最近各个国家也是在,ai 这一块的立法也是越来越多了,第三是你的可解释性,第四是个人的隐私,可能大家其实一直关心的是说,我的工作会不会被 AI 取代,反而这一块在过去几年里面,大家的关心点是在往下降的,是因为确实很多岗位会被 AI 取代,但同时他也创造了更多的岗位,好讲完 AI 的就业之后呢,我们来讲 AI 的教育,下面这个组呢展示的是在北美啊,每一年毕业的,计算机专业的学生的个数,可以看到他一直是在增长,特别是2014年开始,这个增长还是比较快的啊,到了去年为止,他一年能毕业3万个学生,国内应该也是不会比这个数字小啊,然后可以看到 AI 和,机器学习是遥遥领先,有21%的人是这个方向,就是说5个博士毕业里面,其中有一个是做 AI 或者机器学习的,接下来是软件工程,他下面这个图呢表示的是,跟十年前比各个领域的增加的情况,可以看到,ai 和机器学习确实是增长最多的啊,其次是人机交互,减少的最多的是什么,是网络相关以及的编程语言和编辑器,其实我在公司这一块也做,下面这一块也做 做编译器 做硬件,所以导致说我现在确实感觉上,我们要招机器学习的人呢,相对说比较容易,但是要招也很好的啊,编译器啊或者网络这个背景的人呢,相对手比较难一点,毕竟大家的方向,兴趣发生了很大的变化,下面还有一些更细节的分析啊,比如说你 phd 毕业之后呢,一大部分人去的公司,然后还是有30%的人留在了学校,在 cs 和 ai 里面的 phd 里面,女性的占有比例确实还是比较小的,现在只有20%,但是我也知道很,很多学校确实在最近几年,在扩大的女性的招生上面,然后在北美的 AI 的 phd 里面啊,来自于国际学生的个数是60%的样子,当然疫情的缘故是有一点点下降,这所以这里虽然没有统计到底是来,自于哪些国家地区,但基本上是来自亚洲,包括了中国和印度啊,以及啊韩国啊日本啊,好这个就是一个大概的 ai,所以这个就是一个 AI 的就业,和教育的一个大的趋势,所以看到是说不管是在就业上面,还是在学生上面,目前看到还是一个大的增长的趋势,最后一章呢,讲的是 AI 相关的政策和法规,我们来看一下他们到底都有什么,首先看到是每一年,通过相关的法案的增长,所以这个曲线还是一个,基本上这个线性的增长啊,所以可以预测到未来几年,相关的法案应该是会变得更多,然后这些法案里面啊,排名第一的是美国啊,第二是俄罗斯啊,有我有点意外啊第三,是比利时啊,中国其实相对来说,还是通过的并不算多吧,然后这里面是一些法案的一些例子,就是说有什么样的法案到底通过了,其实你仔细看的话会比较有意思,基本上都是一些促进法案,比如说比如说加拿大说我要投一个亿,到我的 AI 的研究院,来促进一个,整个加拿大的 AI 的策略,这是2017年的法案,中国呢在2019年有个法院说我们要促进啊,大数据和人工智能啊,在医疗啊和健康之上的应用,俄罗斯呢在2020年的时候出了一个五年的规划,说我们要怎么样做 ai,英国呢在2020年的时候,允许说他的考试机构,可以用 AI 辅助的方法来进行考核,2020年美国说 nsf 需要去资助一些关于GAN,特别是 deep fake,他出来的结果的研究就是,防止大家来捣乱,防止有人用 deep fake 来捣乱,比利时在2021年说,我们要成立一个伦理的一个委员会,来监督来监督 AI 的使用,法国呢在2021年出了一个草案,是说要监督整个 AI 或者别的技术对于环境的影响,所以基本上就是说,要么就是促进的 AI 的发展,要么就是确保 AI 能够很安全的被使用,所以我们就对这个230页的报告,简单的过了一遍,可以看到他需要5个部分呢,我们先讲呢这里面的一些论文,可以看到我们的论文是越来越多了,所以有看不完的论文,在第二章啊技术上可以看到在各个领域,我们都在过去几年是一直在提升他的性能,这里面主要是有更大的数据,更大的模型以及更强的硬件来支撑的,在很多的应用上已经超越了人类,但是呢,离人类这种高级的意识还有比较遥远,因为 AI 的技术啊现在被大量的部署,所以越来越多的人会,关心 AI 的伦理相关,比如说你的模型的偏见,你的模型的公平性,然后在,就业和教育上面也看到一切都很好啊,去年我们有大量的投资进来,然后大量的学生会毕业,最后是各个国家的政策,也看到在过去几年里面,相关的法案政策也越来越多,但相对来说跟别的领域啊,比如说制造业啊,金融业来说相对来说还是比较少的,但是可以预见,在未来几年会有更多的法案出来,好这就是这一期的内容